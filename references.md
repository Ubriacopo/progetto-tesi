## Self-Supervised Representation Learning & Self Learning
> https://lilianweng.github.io/posts/2019-11-10-self-supervised/

> Contrastive learning: <br>
> https://lilianweng.github.io/posts/2021-05-31-contrastive/

> https://arxiv.org/pdf/2205.05476 
 
> Papers di distillation <br>
> https://github.com/lhyfst/knowledge-distillation-papers

### EEG Processing / Models
> NeuroGPT <br>
> https://arxiv.org/pdf/2311.03764 <br>
> https://github.com/wenhui0206/NeuroGPT

> EEGFormer: 
> https://arxiv.org/pdf/2401.10278 <br>
> Unoffical implementation: https://github.com/FENRlR/EEGformer

> Brain Bert (FAVOURITE):
> https://klab.tch.harvard.edu/publications/PDFs/gk8131.pdf <br>
> Mi piace. Dataset di train era solo SEEG, a noi va bene? BrainBERT (STFT) <br>
> Demo: https://github.com/czlwang/BrainBERT/blob/master/notebooks/demo.ipynb

>How does the model handle different electrodes? Is training shared across them? I was also confused by “First, the Linear (5s time domain input) network is trained once per electrode, for all held-out electrodes.” on page 6 under baselines.
I am particularly interested in the trade-offs of using fewer electrodes and if the authors found better performance on downstream task based on the location of the electrodes, as would be expected.
>
> https://openreview.net/forum?id=xmcYx_reUn6


> Graph EEG: <br> 
> https://arxiv.org/pdf/2310.02152

> https://distill.pub/2021/gnn-intro/

> Multi-modal distillation
> https://arxiv.org/abs/2307.07483

> Library
>https://mne.tools/stable/auto_tutorials/preprocessing/30_filtering_resampling.html