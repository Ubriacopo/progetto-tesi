{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-06T17:17:07.296370Z",
     "start_time": "2025-09-06T17:17:04.345059Z"
    }
   },
   "source": [
    "from common.model.embedding.predefined.cbramod import CBraModFoundationEmbedder\n",
    "from models.EEGAVI.base_EEGAVI.dataset import kd_train_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from models.EEGAVI.base_EEGAVI.EEGAVI import get_default_simple_EEGAVI\n",
    "\n",
    "# model = WorkingEEGAVI(video_kd_size=100, audio_kd_size=100)\n",
    "model = get_default_simple_EEGAVI()\n",
    "ds = kd_train_dataset(\"../../../resources/AMIGOS/processed/spec.csv\")\n",
    "loader = DataLoader(ds, batch_size=2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jacopo/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T17:17:08.177078Z",
     "start_time": "2025-09-06T17:17:07.300626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = next(iter(loader))\n",
    "print(x[0][\"EEGAVI\"][\"eeg\"].shape)\n",
    "x = x[0][\"EEGAVI\"]\n",
    "x[\"kd\"] = True"
   ],
   "id": "f820f64db40297d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "img should be Tensor Image. Got <class 'transformers.image_processing_base.BatchFeature'>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m x = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(x[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mEEGAVI\u001B[39m\u001B[33m\"\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33meeg\u001B[39m\u001B[33m\"\u001B[39m].shape)\n\u001B[32m      3\u001B[39m x = x[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mEEGAVI\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     52\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:292\u001B[39m, in \u001B[36mStackDataset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    290\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    291\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m idx, t_sample \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(indices, list_batch):\n\u001B[32m--> \u001B[39m\u001B[32m292\u001B[39m             t_sample.append(\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[32m    293\u001B[39m tuple_batch: \u001B[38;5;28mlist\u001B[39m[_T_tuple] = [\u001B[38;5;28mtuple\u001B[39m(sample) \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m list_batch]\n\u001B[32m    294\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m tuple_batch\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/common/data/dataset.py:81\u001B[39m, in \u001B[36mKDEEGPdSpecMediaDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     78\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.multi_out_transforms)):\n\u001B[32m     79\u001B[39m     \u001B[38;5;66;03m# Replace where possible\u001B[39;00m\n\u001B[32m     80\u001B[39m     y = dataclasses.replace(x)\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m     o = \u001B[43mcall_pipelines\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmulti_out_transforms\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     83\u001B[39m     out = {\n\u001B[32m     84\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33meeg\u001B[39m\u001B[33m\"\u001B[39m: fix(o.eeg), \u001B[33m\"\u001B[39m\u001B[33mvid\u001B[39m\u001B[33m\"\u001B[39m: fix(o.vid), \u001B[33m\"\u001B[39m\u001B[33maud\u001B[39m\u001B[33m\"\u001B[39m: fix(o.aud), \u001B[33m\"\u001B[39m\u001B[33mtxt\u001B[39m\u001B[33m\"\u001B[39m: fix(o.txt),\n\u001B[32m     85\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmask\u001B[39m\u001B[33m\"\u001B[39m: torch.Tensor([o.eeg \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, o.vid \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, o.aud \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, o.txt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, ]),\n\u001B[32m     86\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33morder\u001B[39m\u001B[33m\"\u001B[39m: (\u001B[33m\"\u001B[39m\u001B[33meeg\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mvid\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33maud\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mtxt\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmask\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     87\u001B[39m     }\n\u001B[32m     88\u001B[39m     outputs[\u001B[38;5;28mself\u001B[39m.multi_out_transforms[mod].name] = out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/common/data/data_point.py:105\u001B[39m, in \u001B[36mcall_pipelines\u001B[39m\u001B[34m(x, pipe_wrapper)\u001B[39m\n\u001B[32m    103\u001B[39m     x.eeg = pipe_wrapper.eeg_transform(x.eeg)\n\u001B[32m    104\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pipe_wrapper.vid_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m105\u001B[39m     x.vid = \u001B[43mpipe_wrapper\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvid_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pipe_wrapper.aud_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    107\u001B[39m     x.aud = pipe_wrapper.aud_transform(x.aud)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001B[39m, in \u001B[36mNormalize.forward\u001B[39m\u001B[34m(self, tensor)\u001B[39m\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) -> Tensor:\n\u001B[32m    270\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    271\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    272\u001B[39m \u001B[33;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    275\u001B[39m \u001B[33;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[32m    276\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:348\u001B[39m, in \u001B[36mnormalize\u001B[39m\u001B[34m(tensor, mean, std, inplace)\u001B[39m\n\u001B[32m    346\u001B[39m     _log_api_usage_once(normalize)\n\u001B[32m    347\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch.Tensor):\n\u001B[32m--> \u001B[39m\u001B[32m348\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    350\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n",
      "\u001B[31mTypeError\u001B[39m: img should be Tensor Image. Got <class 'transformers.image_processing_base.BatchFeature'>"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res = model(x)",
   "id": "612a58085e2a7231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "res[0].shape",
   "id": "56831da139b47cec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')\n",
    "\n",
    "# Love this library !\n",
    "model_graph = draw_graph(model, input_data={\"x\": x}, device='cpu', depth=2, expand_nested=True)\n",
    "model_graph.visual_graph"
   ],
   "id": "a746ec53e49908df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_graph = draw_graph(model, input_data={\"x\": x}, device='cpu', depth=1, expand_nested=True)\n",
    "model_graph.visual_graph"
   ],
   "id": "804524794ea9cb0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "976de2bf12bbae1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Given you still resample video/audio to **64 tokens each (D=768)** and plan to use **EEG as queries (Q)**, your choice—**gated cross-attention**—is spot on. You have genuine token sets on the KV side, so the LM/decoder (driven by EEG queries) can selectively ground into different audio/video regions. No need to switch paradigms.\n",
    "\n",
    "Here’s how I’d set it up, plus a couple of tweaks that make it work reliably.\n",
    "\n",
    "# Keep gated cross-attention (yes)\n",
    "\n",
    "* You’ve got **M=64** tokens per modality → cross-attn has something to attend to. The gate is valuable to:\n",
    "\n",
    "  1. stabilize early training (init near 0),\n",
    "  2. let the model *decide* when EEG should trust A/V context,\n",
    "  3. support future multi-media without refactors.\n",
    "\n",
    "# How to fuse A+V with EEG (QKV design)\n",
    "\n",
    "You have three good options; all keep the “gated-XAttn” flavor:\n",
    "\n",
    "1. **Parallel, per-modality branches + residual add (recommended first)**\n",
    "\n",
    "   * Compute two cross-attn branches in parallel:\n",
    "     $Y_v = \\text{XAttn}(Q_{eeg}, K_v, V_v)$, $Y_a = \\text{XAttn}(Q_{eeg}, K_a, V_a)$\n",
    "   * Gate each branch: $\\hat Y_v = g_v \\cdot P_v(Y_v)$, $\\hat Y_a = g_a \\cdot P_a(Y_a)$\n",
    "   * Fuse by addition: $Q' = Q_{eeg} + \\hat Y_v + \\hat Y_a$\n",
    "   * Use **separate K/V projections per modality** (don’t share) and tiny projection heads $P_v,P_a$ (Linear+Dropout).\n",
    "   * Initialization: set $g_v=g_a\\approx 0$ (e.g., logit init −3 to −5 or just a 0 param passed through tanh/sigmoid).\n",
    "\n",
    "2. **Concat KV with modality tags (simple, cheaper)**\n",
    "\n",
    "   * $K = [K_v; K_a]$, $V = [V_v; V_a]$; add a **modality embedding** to K and V.\n",
    "   * Single gated branch: $Q' = Q_{eeg} + g \\cdot P(\\text{XAttn}(Q_{eeg}, K, V))$\n",
    "   * Add a **length-balancing sampler** if one modality dominates token count.\n",
    "\n",
    "3. **Competitive gating (mixture over modalities)**\n",
    "\n",
    "   * Compute both branches, produce **softmax gates** from $Q_{eeg}$:\n",
    "     $[w_v,w_a]=\\text{softmax}(W\\cdot \\text{Pool}(Q_{eeg}))$\n",
    "   * Fuse: $Q' = Q_{eeg} + w_v\\hat Y_v + w_a\\hat Y_a$\n",
    "   * Helps when one modality is occasionally useless/noisy.\n",
    "\n",
    "All three are fine; (1) is the most robust and easiest to ablate.\n",
    "\n",
    "# Small but important details\n",
    "\n",
    "* **Dim align:** Your video is already 768; audio is 1024→768 via ISAB/PMA projector. Keep **per-modality LayerNorm + Linear** into a **shared d\\_model** (768).\n",
    "* **Positional/temporal cues:** Even after resampling, inject **relative position encodings** per modality (RoPE or learned rel-bias). For videos, you can carry a **frame-index rel bias** into K/V; for audio, a **chunk index**. It helps EEG queries map to correct time neighborhoods.\n",
    "* **Cross-modal time banding (optional):** If you can align EEG time indices to A/V indices, add a **banded attention mask** that softly favors nearby K/V timesteps (e.g., bias matrix with −∞ outside a ±Δ window).\n",
    "* **Depth placement:** Interleave blocks:\n",
    "  `[SelfAttn(EEG) → GatedXAttn(A) → GatedXAttn(V) → MLP] × N`\n",
    "  or use the concat-KV variant once per block to save compute.\n",
    "* **Gates:** Scalar per branch is usually enough. If unstable, switch to **vector gates** (per-channel) with LN on branch outputs.\n",
    "* **Token budget:** 64v + 64a = 128 KV tokens. If EEG query length is large, consider **KV downsample** to 32 each without big quality loss. Keep head dim modest (e.g., 8 heads × 96 = 768).\n",
    "\n",
    "# A clean “braided” block (PyTorch-ish, concise)\n",
    "\n",
    "```python\n",
    "class GatedXAttnBranch(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pdrop=0.0, gate_init=0.0):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=pdrop, batch_first=True)\n",
    "        self.proj = nn.Sequential(nn.Linear(d_model, d_model), nn.Dropout(pdrop))\n",
    "        self.ln_kv = nn.LayerNorm(d_model)\n",
    "        self.gate = nn.Parameter(torch.tensor(gate_init))  # use tanh/sigmoid on forward\n",
    "\n",
    "    def forward(self, q, kv, attn_mask=None, key_padding_mask=None):\n",
    "        kv = self.ln_kv(kv)\n",
    "        y, _ = self.attn(q, kv, kv, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "        y = self.proj(y)\n",
    "        g = torch.tanh(self.gate)  # scalar gate\n",
    "        return g * y\n",
    "\n",
    "class EEGAVBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_heads=8, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.ln_q = nn.LayerNorm(d_model)\n",
    "        self.av_branch_v = GatedXAttnBranch(d_model, n_heads, pdrop, gate_init=0.0)\n",
    "        self.av_branch_a = GatedXAttnBranch(d_model, n_heads, pdrop, gate_init=0.0)\n",
    "        self.mlp = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 4*d_model),\n",
    "                                 nn.GELU(), nn.Dropout(pdrop), nn.Linear(4*d_model, d_model))\n",
    "    def forward(self, q_eeg, kv_v, kv_a, masks=None):\n",
    "        x = self.ln_q(q_eeg)\n",
    "        yv = self.av_branch_v(x, kv_v, attn_mask=masks.get('v_attn') if masks else None)\n",
    "        ya = self.av_branch_a(x, kv_a, attn_mask=masks.get('a_attn') if masks else None)\n",
    "        q = q_eeg + yv + ya\n",
    "        q = q + self.mlp(q)\n",
    "        return q\n",
    "```\n",
    "\n",
    "# When would I *not* use gated X-Attn here?\n",
    "\n",
    "* If you later **collapse A/V to one pooled vector** each—then switch to **Conditional LayerNorm / FiLM** or **prefix tokens**. But as long as you keep **64 tokens**, gated cross-attn is the right hammer.\n",
    "\n",
    "# Quick ablation plan (cheap + decisive)\n",
    "\n",
    "1. **Concat-KV vs parallel branches:** measure validation loss and alignment metrics; parallel almost always wins in noisy-modality settings.\n",
    "2. **Gates on/off:** init gates to 0 vs fixed 1. Gated usually trains smoother and avoids over-conditioning.\n",
    "3. **Token counts:** 64→32 per modality; check that accuracy/CE barely moves while latency drops.\n",
    "4. **Band mask:** no band vs ±4/±8 time windows; expect better temporal grounding.\n",
    "\n",
    "# Reminder\n",
    "\n",
    "Since we moved firmly to **EEG-as-Q, A/V-as-KV with per-modality gates**, we should **update the architecture plot (image.png)** to reflect:\n",
    "\n",
    "* per-modality resampler to 64 tokens,\n",
    "* two gated cross-attn branches into the EEG stream,\n",
    "* optional concat-KV path as a variant.\n",
    "\n",
    "If you want, I can draft the exact node/edge layout text so you can regenerate the figure you keep in `Pasted image.png`.\n"
   ],
   "id": "c2d8f12b7c4c914c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a great question about how to best leverage rich human annotations. Given that you're building a foundation model, I'd recommend **using these annotations as additional modalities** rather than just supervised targets, for several compelling reasons:\n",
    "\n",
    "**Why treat annotations as modalities:**\n",
    "\n",
    "**Richer representation learning**: Valence/arousal/dominance and emotion labels contain structured information about human subjective experience that can help your model learn more nuanced multimodal representations. This is especially valuable for EEG data, where the subjective experience is often what you're trying to decode.\n",
    "\n",
    "**Foundation model flexibility**: As a foundation model, you want representations that can generalize to many downstream tasks. Encoding these annotations as modalities makes them available for various use cases - some might need arousal prediction, others emotion classification, others might benefit from valence-aware representations.\n",
    "\n",
    "**Cross-modal learning**: The model can learn relationships between EEG patterns, video content, and subjective ratings. This could help it understand, for example, how certain EEG signatures relate to high arousal states or specific emotions.\n",
    "\n",
    "**Practical implementation approaches:**\n",
    "\n",
    "1. **Embed the annotations**: Create simple embedders for the numerical ratings (valence/arousal/dominance/familiarity/liking) and categorical emotions\n",
    "2. **Multi-task learning**: Include reconstruction/prediction losses for these modalities alongside your main objectives\n",
    "3. **Masking strategy**: During training, randomly mask some annotation modalities so the model learns to infer missing subjective states\n",
    "\n",
    "**Hybrid approach**: You could do both - use them as modalities for representation learning AND as supervised targets for some training objectives. This gives you the benefits of both approaches.\n",
    "\n",
    "This seems particularly powerful for EEG foundation models since subjective experience is often the ground truth you're ultimately trying to understand from neural signals."
   ],
   "id": "73b16ac51886db98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Your friend is **partly right**, but the key question is: *do you have those annotations at inference time?*\n",
    "\n",
    "Let’s break it down:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. If ratings **are not available at inference** (most foundation-model scenarios)\n",
    "\n",
    "* Treating them as a full **input modality** is risky → it creates **data leakage**: the model learns to depend on signals it won’t see later.\n",
    "* In this case, the safer approach is:\n",
    "\n",
    "  * Use them as **supervision** (multi-task targets).\n",
    "  * Use them as **privileged info** (teacher–student KD).\n",
    "  * Optionally allow them as an **auxiliary branch** with **modality dropout**, so the model learns to work **with or without** them.\n",
    "\n",
    "✅ This keeps the foundation model usable for downstream tasks where ratings aren’t present.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. If ratings **could be available at inference** (e.g., interactive personalization)\n",
    "\n",
    "* Then yes, you *can* treat them as an **additional modality**.\n",
    "\n",
    "  * Encode ratings with a small MLP.\n",
    "  * Fuse them with EEG embeddings (e.g., gated fusion).\n",
    "  * Train with **modality dropout**, so the model doesn’t collapse if they’re missing.\n",
    "\n",
    "✅ This makes sense if your deployment setting sometimes collects quick valence/arousal sliders or self-reports.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Foundation model philosophy\n",
    "\n",
    "* A **foundation model** is meant to be widely usable across settings.\n",
    "* That means you should **not assume rare modalities at inference**.\n",
    "* Instead, ratings are best used to **shape the representation during training**—through **supervision, KD, contrastive alignment**—but not as a dependency at runtime.\n",
    "\n",
    "---\n",
    "\n",
    "🔑 **So, your friend is right only if you know ratings will be present at inference.**\n",
    "For a general-purpose foundation EEG model, it’s safer to keep them as **training signals** rather than a core modality.\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to sketch an updated **architecture diagram** that shows the difference between:\n",
    "\n",
    "1. “ratings as a modality” path (your friend’s suggestion)\n",
    "2. “ratings as supervision + KD” path (my suggestion)?\n"
   ],
   "id": "adf6b98201f0a7a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Your friend makes an excellent point that I should have emphasized more strongly. They're absolutely right about the **inference availability problem** - this is a critical consideration for foundation models.\n",
    "\n",
    "**The core issue**: If your foundation model learns to rely on subjective ratings as input modalities, it becomes less useful for real-world applications where you typically only have EEG/video/audio but no human annotations.\n",
    "\n",
    "**Your friend's suggestions are much more practical:**\n",
    "\n",
    "**Multi-task supervision**: Use ratings as additional loss targets alongside your main objectives. The model learns to predict these from EEG+video+audio, making it better at understanding subjective states without requiring them as input.\n",
    "\n",
    "**Privileged information in KD**: During knowledge distillation, let the teacher model see the annotations but train the student to match the teacher's representations using only EEG+video+audio. This transfers the knowledge without creating dependency.\n",
    "\n",
    "**Auxiliary branch with modality dropout**: This is particularly clever - train with annotations sometimes present, sometimes absent. The model learns robust representations that can leverage annotations when available but don't break without them.\n",
    "\n",
    "**Why this matters for your EEG foundation model:**\n",
    "- Real-world EEG applications rarely have concurrent subjective ratings\n",
    "- You want the model to be deployable in clinical, BCI, or research settings where only physiological + stimulus data exists\n",
    "- The annotations help training but shouldn't become a crutch\n",
    "\n",
    "Your friend's approach gives you the representational benefits of the rich annotation data while maintaining the practical utility of your foundation model. This is definitely the better strategy for real-world deployment.Here’s a concise table of the **best options** for handling your ratings (valence, arousal, dominance, liking, familiarity, emotions) in the context of building a **foundation EEG model**:\n",
    "\n",
    "| **Option**                                 | **How to use ratings**                                                                                                                           | **When it makes sense**                                                                     | **Pros**                                                                     | **Cons / Risks**                                                |\n",
    "| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **Supervision (multi-task)**               | Predict V/A/D (ordinal), emotions (multi-label), liking/familiarity (regression). Heads dropped at inference.                                    | Always (default)                                                                            | Shapes representations with affective info; simple to add.                   | No direct teacher guidance; only works via ground-truth labels. |\n",
    "| **Privileged Info (Teacher–Student / KD)** | Teacher sees EEG+ratings, student only EEG. Distill logits/embeddings.                                                                           | Always, if ratings exist in training data                                                   | Injects human affect structure; student usable without ratings.              | More complex training; need distillation tuning.                |\n",
    "| **Optional Input Modality**                | Encode ratings as a sidecar MLP, fuse with EEG (e.g. gated). Use modality dropout.                                                               | If ratings may sometimes be available at inference (e.g. interactive apps, personalization) | Can boost performance when ratings are present; robust fallback when absent. | Extra parameters; risk of over-reliance if dropout not used.    |\n",
    "| **Pretraining Signals (ratings-free)**     | Self-supervised EEG (masked TS, temporal order), contrastive EEG↔stimulus. Ratings only used indirectly (e.g. pairwise valence/arousal ranking). | Foundation pretraining stage                                                                | Keeps backbone general; ratings guide structure without being inputs.        | Ratings not directly predicted; weaker supervision signal.      |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Recommended mix for a foundation model:**\n",
    "\n",
    "1. **Always**: Supervision (multi-task heads).\n",
    "2. **Additionally**: Teacher–student KD to distill richer structure.\n",
    "3. **Optionally**: Sidecar modality (with dropout) if ratings might be available at inference.\n",
    "4. **Base pretraining**: Keep EEG backbone ratings-free.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to **update the architecture diagram** so it clearly shows these three branches (supervision, KD, optional modality)? That way the design is visually consistent with your `Pasted image.png`.\n"
   ],
   "id": "87433390ba0e48b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That’s a really good catch 👍\n",
    "\n",
    "Yes — if your **sidecar branch directly takes the *ground-truth ratings*** (valence/arousal/dominance, emotions, liking, familiarity) as **inputs** during training **and you don’t have them at inference**, then it *is* a form of **label leakage**. The model would be “cheating,” since it sees the answers while learning.\n",
    "\n",
    "---\n",
    "\n",
    "### How to avoid leakage\n",
    "\n",
    "* **Don’t feed ground-truth ratings as features unless you also expect them at inference.**\n",
    "* Instead, if you want a “ratings sidecar” branch, use:\n",
    "\n",
    "  * **Predicted ratings** from another model (e.g. a lightweight estimator trained separately on stimuli or early EEG features).\n",
    "  * **Noisy / partial user inputs** at inference (e.g. quick sliders, few ratings for personalization).\n",
    "  * **Auxiliary embeddings** derived from clustering ratings across the dataset, so you’re not feeding the raw labels.\n",
    "\n",
    "---\n",
    "\n",
    "### Correct framing\n",
    "\n",
    "* **Training-only → supervision / KD**: Ratings are used to shape representation (safe).\n",
    "* **Inference-time optional modality** → only valid if you **really will have some rating inputs** at test time (e.g. user self-reports).\n",
    "\n",
    "---\n",
    "\n",
    "🔑 So the sidecar isn’t inherently bad — it’s only “leakage” if you train with labels as inputs and then don’t have them at inference. If you want a truly general **foundation EEG model**, keep the ratings in the **supervision / KD role**. Only add them as an **input modality** if your application setting makes them *available at runtime*.\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to propose a clean **two-mode architecture** (one with ratings-as-supervision-only, one with ratings-as-input when available) so you can keep both options without leakage?\n"
   ],
   "id": "e226cf8ad35fa1b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "BUT if someone wants to fintune the model they don't need the vda data right? It is already gated!\n",
    "\n",
    "Correct 👍\n",
    "\n",
    "If you design the **ratings sidecar** with a **gate (modality dropout / fusion switch)**, then during **finetuning**:\n",
    "\n",
    "* A user who **has V/A/D ratings** (or some proxy, like sliders or predicted ratings) can plug them into the sidecar.\n",
    "* A user who **does not have V/A/D ratings** can **ignore the branch** — the EEG backbone and heads are already trained to work *without* it, because you forced that behavior during pretraining with dropout.\n",
    "\n",
    "So:\n",
    "\n",
    "* **Fine-tuning without V/A/D data → totally valid.** The model falls back to the EEG-only pathway.\n",
    "* **Fine-tuning with V/A/D data → possible boost.** The gate lets you fuse them, but they’re not required.\n",
    "\n",
    "This is exactly the reason for the gating trick: it future-proofs the model so that the **foundation checkpoint** can be finetuned **with or without** ratings.\n",
    "\n",
    "👉 In practice, during finetuning, you’d just set the `has_sidecar=False` flag (or drop the input) and the architecture continues to function.\n",
    "\n",
    "Do you want me to annotate your **architecture diagram** so it explicitly shows that the sidecar is *optional at pretraining and optional at finetuning*?\n"
   ],
   "id": "e0f6197aecbb7004"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e0ae73f1a371d676",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
