{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:36:32.886806Z",
     "start_time": "2025-08-19T15:36:32.881455Z"
    }
   },
   "source": [
    "# Imports block\n",
    "import torch\n",
    "from transformers import VivitImageProcessor\n",
    "\n",
    "from common.amigos.dataset import AMIGOSDataset\n",
    "from models.FEEG.base_embedding import BaseEmbedding\n",
    "\n",
    "from torchviz import make_dot\n",
    "from models.FEEG.model import EEGAVI"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Interaction",
   "id": "e4c56ae3bd0dda8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "ds = AMIGOSDataset(\"../../resources/AMIGOS/sampled/AMIGOS_sampled.csv\")\n",
    "ds.__getitem__(1)"
   ],
   "id": "8b2d4d3849d83c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "a = ds.__getitem__(1)\n",
    "for i in a:\n",
    "    if i is None:\n",
    "        print(\"None\")\n",
    "    else:\n",
    "        print(i.shape if isinstance(i, np.ndarray) else len(i))"
   ],
   "id": "dc3b119865a8da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(a[0])",
   "id": "cab6719899787645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.array(a[0]).shape",
   "id": "f8ad996cf1b7bb82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "eeg = BaseEmbedding.get_cbramod_base()",
   "id": "39ecc8ccaa9fc8d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "### Structure\n",
    "We use ```make_dot``` to plot a structure of the actual model. <br>\n",
    "This step is just to see if the shapes match and there were no mistakes on that behalf."
   ],
   "id": "1d7205299df88941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ViViT\n",
    "ViViT accepts only 32 frames sequence inputs. How to operate depends on approach:\n",
    "- Uniform sampling → evenly pick 32 frames across the whole 4 s (good coverage)\n",
    "- Random sampling → randomly pick 32 frames (common in training for augmentation).\n",
    "- Sliding windows → split into multiple 32-frame clips (e.g. 120 frames → 3–4 clips of 32), process each, then average/aggregate.\n",
    "\n",
    "> Sliding window could be what I need for face expressions altough more costy as I need to compute multiple times ViVIT downstream when feeding the video."
   ],
   "id": "fd72d97ee20bf079"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:52:10.366856Z",
     "start_time": "2025-08-19T15:52:07.656151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vivit = BaseEmbedding.get_ViViT_base().model\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "video = torch.randint(low=0, high=256, size=(32, 3, 224, 224))\n",
    "a = processor(list(video), return_tensors=\"pt\")\n",
    "r = vivit(**a)"
   ],
   "id": "80c70c400ad3dee9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:52:44.506652Z",
     "start_time": "2025-08-19T15:52:44.501802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(r.last_hidden_state.shape)\n",
    "# So I could use Patch tokens for mid-fusion from this call? Yes\n",
    "# As 3137 contains: 3136 patch tokens + 1 [CLS] token.\n",
    "# I could opt for Flamingo?"
   ],
   "id": "3f306c52c3827c0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3137, 768])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:57:40.156983Z",
     "start_time": "2025-08-19T15:57:40.148257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "# TODO: HERE\n",
    "# I could work with the full video shape. What does VATE give me back tho?\n",
    "nn.Linear(768, 400)(r.last_hidden_state).shape"
   ],
   "id": "a5e719969c3c55f7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3137, 400])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:35:26.872106Z",
     "start_time": "2025-08-19T15:35:26.868370Z"
    }
   },
   "cell_type": "code",
   "source": "res.last_hidden_state",
   "id": "91089d8a711d3b40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5893,  0.3459,  0.9137,  ..., -0.4443, -0.7825,  0.0405],\n",
       "         [ 0.3717,  0.0462,  0.3935,  ..., -0.4207, -0.1892,  0.0375],\n",
       "         [ 0.5313, -0.5689,  0.3701,  ..., -0.2536, -0.8913,  0.0213],\n",
       "         ...,\n",
       "         [ 0.6091,  0.2223,  0.1393,  ..., -0.0443, -0.8038, -0.1764],\n",
       "         [ 0.2583, -0.0116,  0.7239,  ..., -0.0897, -0.8880, -0.1585],\n",
       "         [ 0.6755,  0.4019,  0.0686,  ..., -0.2835, -0.0257,  0.1094]],\n",
       "\n",
       "        [[ 0.6270,  0.3154,  0.9685,  ..., -0.4629, -0.8222,  0.0963],\n",
       "         [ 0.5641, -0.9205,  0.5163,  ..., -0.5630, -0.0640, -0.0462],\n",
       "         [ 0.7916, -0.4010, -0.3732,  ..., -0.0926, -0.5847,  0.0397],\n",
       "         ...,\n",
       "         [ 0.6397,  0.1159, -0.2169,  ...,  0.1289, -0.4131, -0.1863],\n",
       "         [ 0.9740, -0.2264,  0.1516,  ..., -0.2253, -0.5794,  0.0424],\n",
       "         [ 0.0617, -0.0646,  0.3379,  ..., -0.1600, -0.8507,  0.2688]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = EEGAVI()  # Model initialization\n",
    "\n",
    "# Build input x as:\n",
    "x_vid = torch.randn(2, 32, 3, 224, 224)\n",
    "x_aud = torch.randn(2, 16000)\n",
    "x_tex = None  # We try without text\n",
    "\n",
    "# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)\n",
    "x_eeg = torch.randn(8, 22, 4, 200)\n",
    "\n",
    "y = model((x_eeg, x_vid, x_aud, x_tex))\n",
    "\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)"
   ],
   "id": "a0f31ab9b831aecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "96ccc60ab991c161",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
