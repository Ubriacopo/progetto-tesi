{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:10.382201Z",
     "start_time": "2025-09-02T15:43:05.132237Z"
    }
   },
   "source": [
    "# Imports block\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from transformers import VivitImageProcessor, AutoFeatureExtractor\n",
    "\n",
    "from common.data.amigos.config import CH_NAMES, CH_TYPES\n",
    "from common.data.amigos.dataset import AMIGOSDataset\n",
    "from common.data.amigos.dataset import KDAmigosDataset\n",
    "from common.data.audio.transforms import AudioToTensor, ToMono\n",
    "from common.data.data_point import EEGDatasetTransformWrapper\n",
    "from common.data.video import RegularFrameResampling, VideoToTensor\n",
    "from models.FEEG.layers.base_embedding import ViViTFoundationEmbedder, MiniLMFoundationEmbedder, \\\n",
    "    CBraModFoundationEmbedder\n",
    "from common.model.layers.perceiver_adapter import PerceiverResampler\n",
    "from models.FEEG.model import SimpleEEGAVI\n",
    "from models.FEEG.transforms import ViVitImageProcessorTransform"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:10.388171Z",
     "start_time": "2025-09-02T15:43:10.386299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Finire di far anadare la call al modello\n",
    "#       Sarebbe da vedere quanto ci metto a fare embedding on demand.\n",
    "#       Preprocessing stored non sarebbe male ma poi ci pensiamo in base alle tempistiche.\n",
    "#"
   ],
   "id": "56736dcd69965744",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:11.733519Z",
     "start_time": "2025-09-02T15:43:10.434474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from common.data.eeg.transforms import EEGToTimePatches, EEGResample, EEGToTensor, EEGToMneRawFromChannels\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "aud = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "\n",
    "# Full run to call the model\n",
    "ds = AMIGOSDataset(\n",
    "    \"../../resources/AMIGOS/processed/spec.csv\",\n",
    "    transforms=EEGDatasetTransformWrapper(\n",
    "        name=\"default\",\n",
    "        vid_transform=(\n",
    "            VideoToTensor(),\n",
    "            RegularFrameResampling(32, drop_mask=True),\n",
    "            ViVitImageProcessorTransform()\n",
    "        ),\n",
    "        aud_transform=(\n",
    "            AudioToTensor(),\n",
    "            ToMono(),\n",
    "            v2.Lambda(lambda x: aud(x, padding=True, return_tensors=\"pt\")),\n",
    "        ),\n",
    "        eeg_transform=(\n",
    "            EEGToMneRawFromChannels(CH_NAMES, CH_TYPES),\n",
    "            EEGResample(200, 128),\n",
    "            EEGToTensor(),\n",
    "            EEGToTimePatches(200)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "a = ds[0]"
   ],
   "id": "e26dd3d508f3e306",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:12.564Z",
     "start_time": "2025-09-02T15:43:11.789008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchaudio.transforms import Resample\n",
    "from common.data.audio.transforms import AudioZeroMasking\n",
    "from models.FEEG.transforms import W2VBertFeatureExtractorTransform\n",
    "\n",
    "aud = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "ds = KDAmigosDataset(\n",
    "    \"../../resources/AMIGOS/processed/spec.csv\",\n",
    "    shared_transform=EEGDatasetTransformWrapper(\n",
    "        name=\"base\",\n",
    "        vid_transform=nn.Sequential(\n",
    "            VideoToTensor(),\n",
    "            RegularFrameResampling(32, drop_mask=True),\n",
    "        ),\n",
    "        aud_transform=nn.Sequential(\n",
    "            AudioToTensor(),\n",
    "            Resample(44000, 16000),\n",
    "            AudioZeroMasking(8, 16000),\n",
    "            ToMono(),\n",
    "        ),\n",
    "        eeg_transform=nn.Sequential(\n",
    "            EEGToMneRawFromChannels(CH_NAMES, CH_TYPES),\n",
    "            EEGResample(200, 128),\n",
    "            EEGToTensor(),\n",
    "            EEGToTimePatches(200),\n",
    "            v2.Lambda(lambda x: x.to(\"cuda\")),\n",
    "        )\n",
    "    ),\n",
    "    modality_transforms=[\n",
    "        EEGDatasetTransformWrapper(\n",
    "            name=\"predefined-student\",\n",
    "            vid_transform=nn.Sequential(\n",
    "                ViVitImageProcessorTransform(),\n",
    "                v2.Lambda(lambda x: x.to(\"cuda\")),\n",
    "            ),\n",
    "            aud_transform=nn.Sequential(\n",
    "                # todo pass sampling rate\n",
    "                W2VBertFeatureExtractorTransform(),\n",
    "                v2.Lambda(lambda x: x.to(\"cuda\")),\n",
    "            ),\n",
    "            eeg_transform=(\n",
    "                v2.Lambda(lambda x: {\"x\": x.float()}),\n",
    "            )\n",
    "        ),\n",
    "        EEGDatasetTransformWrapper(name=\"VATE\"),\n",
    "    ]\n",
    ")\n",
    "# Funziona come previsto!\n",
    "b = ds[0]"
   ],
   "id": "368162c9cb94979a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:12.870483Z",
     "start_time": "2025-09-02T15:43:12.697714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model initialization\n",
    "model = SimpleEEGAVI(\n",
    "    text_kd_size=100,\n",
    "    video_kd_size=100,\n",
    "    audio_kd_size=100\n",
    ")"
   ],
   "id": "e4c56ae3bd0dda8a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:13.416943Z",
     "start_time": "2025-09-02T15:43:12.876796Z"
    }
   },
   "cell_type": "code",
   "source": "model.to(\"cuda\")",
   "id": "377b44d11174197a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleEEGAVI(\n",
       "  (video_adapter): EmbedderAdapter(\n",
       "    (adapter): Sequential(\n",
       "      (0): ISAB(\n",
       "        (inference_transformer): CrossTransformerBlock(\n",
       "          (q_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (kv_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (latent_inference_transformer): CrossTransformerBlock(\n",
       "          (q_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (kv_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): PMA(\n",
       "        (transformer_block): CrossTransformerBlock(\n",
       "          (q_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (kv_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (out_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embedder): ViViTFoundationEmbedder(\n",
       "      (base_model): VivitModel(\n",
       "        (embeddings): VivitEmbeddings(\n",
       "          (patch_embeddings): VivitTubeletEmbeddings(\n",
       "            (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): VivitEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x VivitLayer(\n",
       "              (attention): VivitAttention(\n",
       "                (attention): VivitSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (output): VivitSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): VivitIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (intermediate_act_fn): FastGELUActivation()\n",
       "              )\n",
       "              (output): VivitOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (pooler): VivitPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (kd_head): KDHead(\n",
       "      (projection): Linear(in_features=768, out_features=100, bias=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=768, out_features=384, bias=True)\n",
       "  )\n",
       "  (audio_adapter): EmbedderAdapter(\n",
       "    (adapter): Sequential(\n",
       "      (0): PMA(\n",
       "        (transformer_block): CrossTransformerBlock(\n",
       "          (q_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (kv_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embedder): W2VBertFoundationEmbedder(\n",
       "      (base_model): Wav2Vec2BertModel(\n",
       "        (feature_projection): Wav2Vec2BertFeatureProjection(\n",
       "          (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=160, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): Wav2Vec2BertEncoder(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x Wav2Vec2BertEncoderLayer(\n",
       "              (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (ffn1): Wav2Vec2BertFeedForward(\n",
       "                (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): SiLU()\n",
       "                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (self_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (self_attn): Wav2Vec2BertSelfAttention(\n",
       "                (linear_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (linear_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (linear_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (distance_embedding): Embedding(73, 64)\n",
       "              )\n",
       "              (conv_module): Wav2Vec2BertConvolutionModule(\n",
       "                (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                (glu): GLU(dim=1)\n",
       "                (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), groups=1024, bias=False)\n",
       "                (depthwise_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation): SiLU()\n",
       "                (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (ffn2): Wav2Vec2BertFeedForward(\n",
       "                (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): SiLU()\n",
       "                (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (kd_head): KDHead(\n",
       "      (projection): Linear(in_features=1024, out_features=100, bias=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=1024, out_features=384, bias=True)\n",
       "  )\n",
       "  (text_adapter): EmbedderAdapter(\n",
       "    (adapter): Sequential(\n",
       "      (0): PMA(\n",
       "        (transformer_block): CrossTransformerBlock(\n",
       "          (q_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (kv_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mha): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          )\n",
       "          (out_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embedder): MiniLMFoundationEmbedder(\n",
       "      (base_model): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 384)\n",
       "          (token_type_embeddings): Embedding(2, 384)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSdpaSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (kd_head): KDHead(\n",
       "      (projection): Linear(in_features=384, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (eeg_adapter): EmbedderAdapter(\n",
       "    (adapter): Sequential()\n",
       "    (embedder): CBraModFoundationEmbedder(\n",
       "      (base_model): CBraMod(\n",
       "        (patch_embedding): PatchEmbedding(\n",
       "          (positional_encoding): Sequential(\n",
       "            (0): Conv2d(200, 200, kernel_size=(19, 7), stride=(1, 1), padding=(9, 3), groups=200)\n",
       "          )\n",
       "          (proj_in): Sequential(\n",
       "            (0): Conv2d(1, 25, kernel_size=(1, 49), stride=(1, 25), padding=(0, 24))\n",
       "            (1): GroupNorm(5, 25, eps=1e-05, affine=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (4): GroupNorm(5, 25, eps=1e-05, affine=True)\n",
       "            (5): GELU(approximate='none')\n",
       "            (6): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "            (7): GroupNorm(5, 25, eps=1e-05, affine=True)\n",
       "            (8): GELU(approximate='none')\n",
       "          )\n",
       "          (spectral_proj): Sequential(\n",
       "            (0): Linear(in_features=101, out_features=200, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x TransformerEncoderLayer(\n",
       "              (self_attn_s): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "              )\n",
       "              (self_attn_t): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=200, out_features=800, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=800, out_features=200, bias=True)\n",
       "              (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Sequential(\n",
       "          (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projection): Linear(in_features=200, out_features=384, bias=True)\n",
       "  )\n",
       "  (gatedXAttn_layers): ModuleList(\n",
       "    (0-1): 2 x GatedCrossAttentionBlock(\n",
       "      (attn): MaskedCrossAttention(\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (q): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (kv): Linear(in_features=384, out_features=768, bias=False)\n",
       "        (out): Linear(in_features=384, out_features=384, bias=False)\n",
       "      )\n",
       "      (ff): SimpleFeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=384, out_features=768, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=768, out_features=384, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:15.538678Z",
     "start_time": "2025-09-02T15:43:13.427356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "gen = DataLoader(ds, batch_size=8)\n",
    "o = next(iter(gen))"
   ],
   "id": "80ce0c193c2d9dc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../resources/AMIGOS/processed/P01_31_raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19885 =      0.000 ...   155.352 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:43:17.631932Z",
     "start_time": "2025-09-02T15:43:15.614924Z"
    }
   },
   "cell_type": "code",
   "source": "res = model(o[\"predefined-student\"]) # Meh si è rottuto tutto",
   "id": "1e35c434ebb5cba0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# Love this library !\n",
    "model_graph = draw_graph(model, input_data=ds[0][0], device='meta', depth=3,\n",
    "                         expand_nested=True)\n",
    "model_graph.visual_graph"
   ],
   "id": "948bb687eea6e851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds = AMIGOSDataset(\"../../resources/AMIGOS/sampled/AMIGOS_sampled.csv\")\n",
    "record = ds.__getitem__(1)"
   ],
   "id": "8b2d4d3849d83c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "record.__dict__.keys()",
   "id": "6a09f2adf243e800",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Todo make them return tensors\n",
    "for key in record.__dict__.keys():\n",
    "    if record.__getattribute__(key) is None:\n",
    "        print(\"None\")\n",
    "    else:\n",
    "        i = record.__getattribute__(key)\n",
    "        print(\"key: \" + key)\n",
    "        print(str(i.shape) if isinstance(i, np.ndarray) else \"arr::len::\" + str(len(i)))\n",
    "        print(\"\")  # Newline for clarity\n",
    "\n",
    "eeg = record.eeg"
   ],
   "id": "dc3b119865a8da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "### Structure\n",
    "We use ```make_dot``` to plot a structure of the actual model. <br>\n",
    "This step is just to see if the shapes match and there were no mistakes on that behalf."
   ],
   "id": "1d7205299df88941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ViViT\n",
    "ViViT accepts only 32 frames sequence inputs. How to operate depends on approach:\n",
    "- Uniform sampling → evenly pick 32 frames across the whole 4 s (good coverage)\n",
    "- Random sampling → randomly pick 32 frames (common in training for augmentation).\n",
    "- Sliding windows → split into multiple 32-frame clips (e.g. 120 frames → 3–4 clips of 32), process each, then average/aggregate.\n",
    "\n",
    "> Sliding window could be what I need for face expressions altough more costy as I need to compute multiple times ViVIT downstream when feeding the video."
   ],
   "id": "fd72d97ee20bf079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vivit = ViViTFoundationEmbedder()\n",
    "video = torch.randint(low=0, high=256, size=(32, 3, 224, 224))\n",
    "\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "a = processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "non_reshaped = vivit.base_model(a.pixel_values)\n",
    "r = vivit(**a)"
   ],
   "id": "80c70c400ad3dee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "o = vivit.base_model(**a)",
   "id": "3b87538ddaf0816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "[str(key) + \" \" + str(o[key].shape) for key in o.keys()]",
   "id": "70970c57f1e4f95f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "r.shape",
   "id": "f0cd2da9f278e833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "o = vivit.base_model(**a)",
   "id": "5b166a7ba5ef9fdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vivit.base_model.config.tubelet_size[-1]",
   "id": "7b35ed30da3846d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "o.pooler_output.shape",
   "id": "6d32ee73a9265574",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  ViViT + Perceiver",
   "id": "392b61533d004bae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Shape of video after ViViT:\" + str(r.shape))\n",
    "resampler_video = PerceiverResampler(768, 2, max_num_frames=32, max_num_media=32)(r)\n",
    "print(\"After OpenFlamingo: \" + str(resampler_video.shape))"
   ],
   "id": "e474b137c7afec0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fuse the time steps with the latent dimension space\n",
    "R_v = rearrange(resampler_video, \"b t l d -> b (t l) d\")\n",
    "R_v.shape"
   ],
   "id": "3361a45bc0dd8b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f8ceefc976feb22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WavLM",
   "id": "8818520364b8013e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.FEEG.layers.base_embedding import W2VBertFoundationEmbedder\n",
    "\n",
    "audio_processor = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "audio = audio_processor(torch.randn(16000), padding=True, return_tensors='pt')\n",
    "wavlm = W2VBertFoundationEmbedder()"
   ],
   "id": "49f086adc4bd9f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = wavlm.base_model(**audio)\n",
    "# odict_keys(['last_hidden_state', 'extract_features'])"
   ],
   "id": "412110255197750c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a[\"last_hidden_state\"].shape",
   "id": "e89c60f8a4cc91c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wavlm",
   "id": "7fd671dac99b3737",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = wavlm(**audio)\n",
    "print(\"Shape after WavLM: \" + str(y.shape))\n",
    "resampled = PerceiverResampler(1024, 2)(y)\n",
    "print(\"Simple perceiver:\" + str(resampled.shape))"
   ],
   "id": "bb53d739db3cf8ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "R_a = rearrange(resampled, \"b t l d -> b (t l) d\")\n",
    "R_a.shape"
   ],
   "id": "bb72709c905b58a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The gated attn is fed with the concatenation of the aux embeddings.\n",
    "torch.cat([R_v, R_a], dim=1).shape"
   ],
   "id": "678c24e4e65e7e08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MiniLM",
   "id": "4411085b4f564e1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "minilm = MiniLMFoundationEmbedder()",
   "id": "433ebf8acee409dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "inputs = \"This is a text test\"\n",
    "minilm_processor = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "a = minilm_processor(inputs, padding=True, truncation=True, return_tensors='pt')"
   ],
   "id": "ce46d3f59504c6ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a",
   "id": "26ecefcbf31661ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = minilm(**a)\n",
    "res.shape"
   ],
   "id": "a36d2e2ad0cbbf7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "text_resampled = PerceiverResampler(384, 2)(res)\n",
    "R_t = rearrange(text_resampled, \"b t l d -> b (t l) d\")\n",
    "print(R_t.shape)  # Shape mismatch for building the input\n",
    "\n",
    "# For mismatch simply project to correct feature space.\n",
    "o = nn.Linear(384, 768)(R_t)\n",
    "o.shape"
   ],
   "id": "42f7450a1d7b946a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CBraMod\n",
   "id": "9222992b2dda7033"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cbramod = CBraModFoundationEmbedder()\n",
    "# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)\n",
    "x_eeg = torch.randn(1, 22, 4, 200)\n",
    "res = cbramod(x=x_eeg)\n",
    "\n",
    "x_eeg.shape"
   ],
   "id": "a2077d18d388a1bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.signal import resample_poly\n",
    "import mne\n",
    "\n",
    "# TODO move this stuff elsewhere\n",
    "# TODO Should I resample to 200Hz? Like the way it does in CBraMod. Non sembra ideale. Meglio paddare.\n",
    "\n",
    "x_eeg = torch.Tensor(eeg.T).unsqueeze(0)\n",
    "print(x_eeg.shape)  # Sampled has fused timesteps with D. We can retrieve it the right way:\n",
    "\n",
    "# 5s time steps as per setup. This will change\n",
    "x_eeg = rearrange(x_eeg, \"b c (t d) -> b c t d\", t=5)\n",
    "mne_eeg = mne.io.RawArray(eeg.T, info=mne.create_info(sfreq=128,\n",
    "                                                      ch_names=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\",\n",
    "                                                                \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\"]))\n",
    "\n",
    "print(x_eeg.shape)\n",
    "\n",
    "# TODO masking servita per segments corti (ora sono fissi)\n",
    "x_eeg = resample_poly(x_eeg, up=25, down=16, axis=-1)  # shape: [B, C, T * 25/16]\n",
    "x_eeg = torch.Tensor(x_eeg)\n",
    "# mask = torch.ones([17, 5, 128])\n",
    "# mask = F.pad(mask, (0, 200 - mask.shape[-1]))\n",
    "# print(mask.shape)  # Damn mi sa che la maschera era per timesteps only! todo caprie"
   ],
   "id": "cc697ab06ba74f02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mne_eeg.plot()",
   "id": "f8de0ddf2f2dbe0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mne_eeg.resample(200, method=\"polyphase\", npad=\"auto\")\n",
    "mne_eeg.plot()"
   ],
   "id": "f7ca71281d0842f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_eeg = torch.Tensor(mne_eeg.get_data()).unsqueeze(0)\n",
    "x_eeg = rearrange(x_eeg, \"b c (t d) -> b c t d\", t=5)"
   ],
   "id": "67183e9847f77784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x_eeg.shape",
   "id": "49c7f2864c962f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cbramod = CBraModFoundationEmbedder()\n",
    "# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)\n",
    "res = cbramod(x=x_eeg, mask=None)"
   ],
   "id": "42da9e34958e5bdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res.shape\n",
    "# todo make mask x object for cbramod"
   ],
   "id": "2ef078a970b4d818",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Model",
   "id": "1802f03537b89ca0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Load an EEG sample",
   "id": "940398bdcd740833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model initialization\n",
    "model = EEGAVI(\n",
    "    resampler_depth=2,\n",
    "    text_kd_size=100,\n",
    "    video_kd_size=100,\n",
    "    audio_kd_size=100\n",
    ")\n",
    "\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "# Build input x as:\n",
    "video = torch.randint(low=0, high=256, size=(32, 3, 224, 224))\n",
    "x_vid = processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "x_aud = processor(torch.randn(1, 16000), return_tensors=\"pt\")\n",
    "\n",
    "x_tex = None  # We try without text"
   ],
   "id": "5c02239400a7acea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x_vid.keys()",
   "id": "4dfb0e76584611a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y = model(({\"x\": x_eeg}, x_vid, x_aud, x_tex))",
   "id": "eda8b00b532a2f4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y",
   "id": "23525b570d4f1221",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y[0].shape",
   "id": "94d154ef279d76a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y[0].shape",
   "id": "5fa8a66a65943d34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "({\"x\": x_eeg}, x_vid, x_aud, x_tex)",
   "id": "452f6cfcedf9964e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# Love this library !\n",
    "model_graph = draw_graph(model, input_data={\"x\": ({\"x\": x_eeg}, x_vid, x_aud, x_tex)}, device='meta', depth=3,\n",
    "                         expand_nested=True)\n",
    "model_graph.visual_graph"
   ],
   "id": "10481111f011554f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "# Love this library !\n",
    "model_graph = draw_graph(model, input_data={\"x\": ({\"x\": x_eeg}, x_vid, x_aud, x_tex)}, device='meta', depth=3,\n",
    "                         expand_nested=True)\n",
    "model_graph.visual_graph"
   ],
   "id": "d8e2c26ebadbb55c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameter count is: {total_params}. Trainable parameter count is: {trainable}\")"
   ],
   "id": "4bfc6328b8d96225",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bucket = defaultdict(int)\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        top = name.split('.', 1)[0]  # e.g., 'fusion', 'base_video'\n",
    "        bucket[top] += p.numel()\n",
    "\n",
    "for k, v in sorted(bucket.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"{k:24s} {v:,}\")"
   ],
   "id": "8d9b3e14452ecd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# next this:\n",
    "# todo\n",
    "# review cross attention + rerun per vedere se si è rotto.\n",
    "# Trainer + loss + KD"
   ],
   "id": "ac88f09a92bd73e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "483223e33bfc2313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "4101327ac0efaa27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import layers.base_layers\n",
    "import importlib\n",
    "\n",
    "importlib.reload(layers.base_layers)\n",
    "from layers.base_layers import QueryEEGFormer\n",
    "\n",
    "former = QueryEEGFormer(200, 384, 10, 20)\n",
    "\n",
    "former(res).shape"
   ],
   "id": "413eb89d29ef1f7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great question. In OpenFlamingo-style **MaskedCrossAttention**, `media_locations` is a boolean **over the query sequence** that marks **where a new media item begins**. The layer then maps each query time step to its **most recent** media item (or **all previous** ones, depending on the flag). In the original Flamingo, these `True` positions are where the special `<image>` tokens appear in the text. ([GitHub][1], [Hugging Face][2])\n",
    "\n",
    "Here’s how to build it for your EEG-as-query setup.\n",
    "\n",
    "---\n",
    "\n",
    "# What the mask means\n",
    "\n",
    "Given:\n",
    "\n",
    "* `qo`: your query tokens, shape **(B, Tq, Dq)** (e.g., EEG tokens over time; if you use *time×channel* then Tq = T\\*C),\n",
    "* `kvo`: your media latents, shape **(B, T\\_item, n, Dk)** (T\\_item = number of media **items** — images or clips),\n",
    "\n",
    "you need **`media_locations: (B, Tq)`** such that:\n",
    "\n",
    "* `media_locations[b, t] == True` exactly at timesteps `t` where **item boundaries** occur (the first time each media item is introduced).\n",
    "* Inside the layer, `q_time = cumsum(media_locations)` yields the “index” of the latest available media for each query step; queries before the first `True` have `q_time=0` and therefore attend to **no media** when `only_attend_immediate_media=True`. ([Hugging Face][2])\n",
    "\n",
    "---\n",
    "\n",
    "# Typical cases & code\n",
    "\n",
    "## Case A — one media item for the whole EEG window (most common)\n",
    "\n",
    "You want every query to be allowed to attend to that one item.\n",
    "\n",
    "```python\n",
    "def media_locs_single_item(B, Tq, device):\n",
    "    m = torch.zeros(B, Tq, dtype=torch.bool, device=device)\n",
    "    m[:, 0] = True            # item “introduced” at t=0\n",
    "    return m\n",
    "```\n",
    "\n",
    "Call:\n",
    "\n",
    "```python\n",
    "media_locations = media_locs_single_item(B=qo.size(0), Tq=qo.size(1), device=qo.device)\n",
    "out = xattn(qo, kvo, media_locations=media_locations, use_cached_media=False)\n",
    "```\n",
    "\n",
    "> If you pass `media_locations=None`, set `use_cached_media=True` to bypass the shape assert and pretend every query happens **after** the last media item (useful for caching; all queries attend to the cached item). ([Hugging Face][2])\n",
    "\n",
    "## Case B — multiple media items on a timeline\n",
    "\n",
    "You have `T_item` clips/images and you know their **start indices** on the EEG query timeline: `starts = [t0, t1, ..., t_{T_item-1}]` with `0 ≤ t_i < Tq`.\n",
    "\n",
    "```python\n",
    "def build_media_locations_from_starts(B, Tq, starts, device):\n",
    "    m = torch.zeros(B, Tq, dtype=torch.bool, device=device)\n",
    "    idx = torch.as_tensor(starts, device=device, dtype=torch.long)\n",
    "    m[:, idx] = True\n",
    "    return m\n",
    "\n",
    "# Example: three clips starting at 0, 40, 80\n",
    "media_locations = build_media_locations_from_starts(B, Tq, [0, 40, 80], qo.device)\n",
    "out = xattn(qo, kvo, media_locations=media_locations, use_cached_media=False)\n",
    "```\n",
    "\n",
    "* With `only_attend_immediate_media=True`, each query `t` attends **only to** the most recent item (mask uses equality on that cumulative index).\n",
    "* With it set **False**, a query attends to **all previous items** (mask uses ≥). ([Hugging Face][2])\n",
    "\n",
    "## Case C — your queries are **time×channel** tokens\n",
    "\n",
    "If you flattened `(B, C, T, D)` into `(B, T*C, D)` **time-major**, just **repeat the time mask across channels**:\n",
    "\n",
    "```python\n",
    "# base time mask of shape (B, T)\n",
    "m_time = media_locs_single_item(B, T, qo.device)         # or build from starts\n",
    "# expand to (B, T*C) to match qo flattened as (B, T*C, D)\n",
    "media_locations = m_time.repeat_interleave(repeats=C, dim=1)\n",
    "```\n",
    "\n",
    "This way, all channel tokens at time `t` share the same “item index”.\n",
    "\n",
    "---\n",
    "\n",
    "# Sanity checks\n",
    "\n",
    "* `media_locations.shape == (B, Tq)` must match the **query length** (not `kvo`).\n",
    "* If some queries occur **before** any item is introduced (all `False` up to `t`), they won’t be allowed to attend when `only_attend_immediate_media=True`. If that’s not desired, introduce your first item at `t=0` (as in Case A).\n",
    "* If you concatenate multiple modalities in `kvo`, concatenate along **`n`** (the latent axis), **not** along `T_item`. All modalities in a given **item slot** are available at the same time index. (If their item counts differ, pad/align to a shared `T_item`.)\n",
    "\n",
    "---\n",
    "\n",
    "# Minimal drop-in utility\n",
    "\n",
    "```python\n",
    "def make_media_locations(Tq, item_starts, B=1, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    item_starts: iterable of length T_item with start indices (0-based) for each media item\n",
    "    returns: (B, Tq) boolean mask\n",
    "    \"\"\"\n",
    "    m = torch.zeros(B, Tq, dtype=torch.bool, device=device)\n",
    "    if len(item_starts) == 0:\n",
    "        # fallback: treat as already cached\n",
    "        return m  # but then call xattn with use_cached_media=True\n",
    "    idx = torch.as_tensor(item_starts, device=device, dtype=torch.long)\n",
    "    m[:, idx.clamp_(0, Tq-1)] = True\n",
    "    return m\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# How this mirrors OpenFlamingo\n",
    "\n",
    "In OpenFlamingo, text tokens contain `<image>` markers; `media_locations[b, t]` is **True** exactly where those tokens sit. The cross-attn layer converts that boolean vector into a cumulative “media index per query” and masks attention to either the **immediately preceding** or **all previous** media items. You’re doing the same—but your **queries are EEG timesteps** instead of text tokens. ([GitHub][1], [Hugging Face][2])\n",
    "\n",
    "If you share how you segment your EEG window vs. media items (e.g., one clip per window or many), I can give you the exact `item_starts` to pass for your shapes.\n",
    "\n",
    "[1]: https://github.com/mlfoundations/open_flamingo \"GitHub - mlfoundations/open_flamingo: An open-source framework for training large multimodal models.\"\n",
    "[2]: https://huggingface.co/Infi-MM/infimm-hd/raw/af3d978efd5292624c1ce80dee50bf8cb6d11275/flamingo_lm.py?utm_source=chatgpt.com \"https://huggingface.co/Infi-MM/infimm-hd/raw/af3d9...\"\n"
   ],
   "id": "eae135fffeea14fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great questions — you’re basically at the “add the heads & poolers” step.\n",
    "\n",
    "## 1) From `[b, 1, 64, 384]` to `[b, 100]` for KD\n",
    "\n",
    "Think of the Perceiver output as **64 latent tokens** of width 384 (the leading `1` is just “num media streams”). For KD you just need a **global vector** per sample, then a **logit layer**.\n",
    "\n",
    "**Simple & solid recipe**\n",
    "\n",
    "```python\n",
    "x = perceiver_out            # [b, 1, 64, 384]\n",
    "x = x.squeeze(1)             # [b, 64, 384]\n",
    "\n",
    "# pool tokens -> one vector\n",
    "pooled = x.mean(dim=1)       # [b, 384]  (or use attention pooling; see below)\n",
    "\n",
    "# KD head\n",
    "kd_logits = nn.Linear(384, 100)(pooled)   # [b, 100]\n",
    "```\n",
    "\n",
    "**Attention pooling (often a tiny win)**\n",
    "\n",
    "```python\n",
    "class AttnPool(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(d, 1)  # token score\n",
    "    def forward(self, x):             # x: [b, T, d]\n",
    "        a = torch.softmax(self.gate(x).squeeze(-1), dim=1)  # [b, T]\n",
    "        return (a.unsqueeze(1) @ x).squeeze(1)              # [b, d]\n",
    "```\n",
    "\n",
    "Then do `pooled = AttnPool(384)(x)` and the same `Linear(384→100)`.\n",
    "\n",
    "**KD loss reminder (PyTorch)**\n",
    "\n",
    "```python\n",
    "T = 2.0  # temperature\n",
    "loss_kd = torch.nn.functional.kl_div(\n",
    "    torch.log_softmax(kd_logits / T, dim=-1),\n",
    "    torch.softmax(teacher_logits / T, dim=-1),\n",
    "    reduction=\"batchmean\"\n",
    ") * (T * T)\n",
    "```\n",
    "\n",
    "(Ensure the teacher’s 100 categories match your KD head’s 100.)\n",
    "\n",
    "> Tip: Keep the KD head **separate** from whatever head you use after cross-attention. Share the backbone, have different heads, and combine losses: `L = α·L_kd + β·L_ssl + …`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) What “patches” are (and why you have 85 of them)\n",
    "\n",
    "A **patch** is just a **local chunk turned into one token**.\n",
    "\n",
    "* For images: a 16×16 crop → 1 token.\n",
    "* For EEG/time: a **window of samples** (e.g., 200 ms with some stride) — possibly per channel or after a channel-mixing layer — → 1 token.\n",
    "\n",
    "So `[b, 85, 384]` means you currently have **85 tokens** (85 local windows) each with a 384-D embedding. The exact 85 comes from your patch/window size, stride, and any channel grouping; it’s the count of windows your embedding layer produced.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Are `[b, 85, 384]` “good embeddings”, or reduce to `[b, E]`?\n",
    "\n",
    "Both are useful — pick based on the next step:\n",
    "\n",
    "* **Keep sequence `[b, 85, 384]`** if you’ll:\n",
    "\n",
    "  * do more transformer layers / cross-attention,\n",
    "  * use token-level losses (e.g., masked modeling, alignment at specific times).\n",
    "\n",
    "* **Reduce to a vector `[b, E]`** if you need:\n",
    "\n",
    "  * clip-level classification, retrieval, KD, logging to disk, etc.\n",
    "\n",
    "**Common ways to reduce**:\n",
    "\n",
    "* Mean pooling: fast, stable.\n",
    "* Attention pooling (above): learn to weight important tokens.\n",
    "* CLS token: add one learnable token and read it out.\n",
    "* Small MLP head: `LayerNorm → Linear(384→E) → GELU → Linear(E→E)` if you want a specific E (e.g., 256).\n",
    "\n",
    "You can also keep **both**: pass the sequence forward, and in parallel keep a pooled vector for KD/metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### Minimal, practical wiring\n",
    "\n",
    "* **KD head (from Perceiver)**: `squeeze → pool → Linear(384→100)`.\n",
    "* **Fusion path**: keep `[b, 85, 384]` for cross-attention; pool **after** fusion if a global output is needed.\n",
    "* If you ever change widths, add a tiny adapter: `LayerNorm → Linear(in→out)`.\n",
    "\n",
    "That’s it — you don’t need to reshape into anything fancy for KD; just pool then a single linear layer.\n"
   ],
   "id": "43cf54845050c65f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ccc4d8101e99727b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Yes—it’s totally fine. You don’t need an EEG teacher to use a CLIP-style objective with your **EEG-anchored fused stream**. You can contrast the fused embedding against **video/audio/text teacher embeddings** only. Here are solid ways to do it (pick one):\n",
    "\n",
    "## A) Fused → multi-teacher contrast (simple & effective)\n",
    "\n",
    "* Pool your fused tokens $(b,T,D)$ to a global vector $z_f\\in\\mathbb{R}^D$ (mean/attn pool + L2-norm).\n",
    "* For each available teacher $t_m$ $(m\\in\\{\\mathrm{vid},\\mathrm{aud},\\mathrm{text}\\})$, L2-normalize and do retrieval **from $z_f$ to $t_m$**.\n",
    "* Because you have **multiple positives** for the same $z_f$, use a **Sigmoid (SigLIP-style) pairwise loss** or multi-positive InfoNCE.\n",
    "\n",
    "**Loss (Sigmoid, per teacher, summed over m):**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{sig}}^{(m)}=\\text{BCE}(\\sigma(s_{ii}^{(m)}),1)+\\!\\!\\sum_{j\\ne i}\\!\\text{BCE}(\\sigma(s_{ij}^{(m)}),0),\\quad\n",
    "s_{ij}^{(m)}=\\tfrac{1}{\\tau_m} z_f^{(i)}\\!\\cdot\\! t_m^{(j)}\n",
    "$$\n",
    "\n",
    "Use a **separate temperature $\\tau_m$** (or learnable `logit_scale_m`) per teacher.\n",
    "\n",
    "**Why this works:** your fused vector has three paired targets (v/a/t) for the same sample; in-batch others are negatives. No EEG teacher needed.\n",
    "\n",
    "## B) Tiny per-teacher heads (avoid space mismatch)\n",
    "\n",
    "Teacher spaces differ. Two tidy options:\n",
    "\n",
    "* Map **teachers → shared space**: learn small adapters $A_m$ so $s_m=A_m t_m$ live in the same space as $z_f$; do A) on $\\{s_m\\}$.\n",
    "* Map **fused → each teacher space**: heads $h_m$ predict $\\hat t_m=h_m(z_f)$; train with cosine/MSE or CRD vs. $t_m$. Keep only $z_f$ at inference.\n",
    "\n",
    "## C) Structure-aware KD (extra stability, optional)\n",
    "\n",
    "* **CRD:** contrast $z_f$ against $\\{t_m\\}$ with in-batch teacher negatives (transfers neighborhood structure).\n",
    "* **RKD/SPKD:** match pairwise geometry (distances/similarities) between $\\{z_f\\}$ and $\\{t_m\\}$ per batch.\n",
    "* **EMA self-teacher for fused:** keep a momentum copy of your fused encoder and add a small cosine term $1-\\cos(z_f,\\;\\text{sg}(z_f^{\\text{ema}}))$ to reduce drift toward any single teacher.\n",
    "\n",
    "## Practical recipe (what I’d start with)\n",
    "\n",
    "* L2-normalize all embeddings.\n",
    "* Attention-pool fused tokens → $z_f$.\n",
    "* Per-teacher learnable `logit_scale_m` and **Sigmoid pairwise** loss; weight by trust, e.g. $\\lambda_v=0.4,\\lambda_a=0.35,\\lambda_t=0.25$.\n",
    "* Add tiny **teacher→shared** adapters $A_m$ before contrast (or fused→teacher heads—either works; adapters keep $z_f$ unified).\n",
    "* Optional small **cosine KD** and **RKD** for stability.\n",
    "\n",
    "### Minimal PyTorch sketch (multi-teacher Sigmoid)\n",
    "\n",
    "```python\n",
    "# zf: (B,D) fused pooled; tv, ta, tt: (B,D) teachers; all L2-normalized\n",
    "def siglip(z, t, logit_scale):\n",
    "    sim = logit_scale * (z @ t.t())               # (B,B)\n",
    "    pos = sim.diag()\n",
    "    pos_loss = torch.nn.functional.binary_cross_entropy_with_logits(pos, torch.ones_like(pos))\n",
    "    neg = sim[~torch.eye(sim.size(0), dtype=torch.bool, device=sim.device)]\n",
    "    neg_loss = torch.nn.functional.binary_cross_entropy_with_logits(neg, torch.zeros_like(neg))\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "loss = (\n",
    "    0.4 * siglip(zf, tv, torch.exp(logit_scale_v)) +\n",
    "    0.35 * siglip(zf, ta, torch.exp(logit_scale_a)) +\n",
    "    0.25 * siglip(zf, tt, torch.exp(logit_scale_t))\n",
    ")\n",
    "# optional: + 0.2 * (1 - (zf * zf_ema).sum(-1).mean())  # EMA self-distill\n",
    "```\n",
    "\n",
    "**Bottom line:** You can train your single fused encoder perfectly well using only **video/text/audio teachers**. Align the fused embedding to those spaces (contrastive + light KD), and you’re good—no EEG teacher required.\n"
   ],
   "id": "54e9c92d570b8c7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "700ec9645ccc2216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great q — and you’re thinking about exactly the right failure mode.\n",
    "\n",
    "## First: your loss intuition (tiny correction)\n",
    "\n",
    "Don’t do BCE on “vector differences.” Do **BCE on pairwise similarities** (dot-product or cosine) between your fused embedding $z_f$ and teacher embeddings $t_m$ (m ∈ {video, audio, text}). That’s the **SigLIP** style loss:\n",
    "\n",
    "$$\n",
    "S^{(m)} = \\mathrm{logit\\_scale}_m \\cdot (Z_f\\,T_m^\\top)\\quad;\\quad\n",
    "\\mathcal{L}^{(m)}=\\text{BCEWithLogits}(S^{(m)}, Y),\\ \\ Y_{ii}=1,\\ Y_{i\\neq j}=0\n",
    "$$\n",
    "\n",
    "So yes: positives are $(z_f^i,t_m^i)$; all other pairs in the batch are negatives. This is exactly the “sigmoid pairwise” contrastive setup, robust to multi-positives and batch size. ([CVF Open Access][1], [arXiv][2])\n",
    "\n",
    "## Does this guarantee you learn from EEG?\n",
    "\n",
    "**Not by itself.** With $z_f$ produced by EEG-queries over KV={video,audio,text}, the model could minimize the teacher loss by mostly **copying KV information**, using EEG only as a weak pointer. Flamingo’s gated cross-attention makes that easy if unconstrained. ([NeurIPS Proceedings][3], [SciSpace][4])\n",
    "\n",
    "### How to make EEG *causally matter*\n",
    "\n",
    "Add one or more of these (lightweight) constraints:\n",
    "\n",
    "1. **Modality dropout / masking (strongest & simple).**\n",
    "   Randomly zero / drop each non-EEG modality during training (and sometimes drop **all** KVs), *while still applying the teacher contrastive loss*. This forces the model to map **EEG→teacher space** when auxiliaries are missing, preventing a KV-only shortcut. You can ramp the drop prob from 0.2→0.6 over training. Variants of modality dropout are standard to avoid over-reliance on any stream. ([arXiv][5], [CVF Open Access][6])\n",
    "\n",
    "2. **Capacity bottleneck on cross-modal flow.**\n",
    "   Limit KV→EEG bandwidth with **bottleneck tokens / Perceiver-style resampler** (small latent count), or MBT-style fusion bottlenecks. This prevents simply piping the whole teacher signal through to $z_f$. ([arXiv][7], [Google Research][8])\n",
    "\n",
    "3. **EEG-only self-supervision (small weight).**\n",
    "   Add an EEG-only SSL term so $z_f$ must retain EEG information: e.g., **CPC** across time on EEG tokens, or masked-reconstruction (MAE-style) on EEG. Even a tiny weight (λ≈0.1) is enough to anchor learning in EEG. ([arXiv][9])\n",
    "\n",
    "4. **Regularize the gate / cross-attn.**\n",
    "   Put an $\\ell_1$ penalty on the **gating scalars** (from the gated cross-attn) or on the cross-attn output norm, to discourage always-open gates. Keep teacher encoders **frozen** (no gradients into them).\n",
    "\n",
    "5. **Mismatched-KV negatives (optional).**\n",
    "   Do a second forward pass where you **shuffle KVs** across samples; add a term that pushes $(z_f, t_m)$ apart under mismatched KVs. If EEG isn’t informative, the model will fail this check.\n",
    "\n",
    "### Quick recipe that works in practice\n",
    "\n",
    "* Use **SigLIP** losses fused→{video,audio,text} (separate temperatures per modality). ([CVF Open Access][1])\n",
    "* **Modality dropout** on KV with scheduled probability; once per batch, also run an **EEG-only** pass and apply the same teacher loss. ([arXiv][5])\n",
    "* Keep a tiny **EEG-SSL** term (CPC or masked-recon) on the EEG tokens. ([arXiv][9])\n",
    "* Use a small **bottleneck** (e.g., 16–32 resampled tokens) for KV. ([arXiv][7])\n",
    "\n",
    "### How to verify EEG is used (sanity checks)\n",
    "\n",
    "* **Leave-one-modality-out** eval: drop KVs at test time and measure the retrieval/ downstream drop; the model should still perform > chance.\n",
    "* **Saliency/grad norms** wrt EEG tokens should be non-trivial.\n",
    "* **Performance vs. drop-prob** curves: if training worked, curves degrade gracefully when KVs are removed.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom line:** Your BCE/contrastive idea is right—but it **doesn’t** ensure EEG contributes unless you block shortcuts. Combine **multi-teacher SigLIP** with **modality dropout** (plus a tiny EEG-only SSL and/or **bottlenecks**) to make $z_f$ genuinely EEG-grounded while still leveraging video/audio/text. ([CVF Open Access][1], [arXiv][5])\n",
    "\n",
    "[1]: https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.pdf?utm_source=chatgpt.com \"Sigmoid Loss for Language Image Pre-Training\"\n",
    "[2]: https://arxiv.org/abs/2303.15343?utm_source=chatgpt.com \"Sigmoid Loss for Language Image Pre-Training\"\n",
    "[3]: https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf?utm_source=chatgpt.com \"🦩 Flamingo: a Visual Language Model for Few-Shot Learning\"\n",
    "[4]: https://scispace.com/pdf/flamingo-a-visual-language-model-for-few-shot-learning-2552lk6m.pdf?utm_source=chatgpt.com \"arXiv:2204.14198v2 [cs.CV] 15 Nov 2022\"\n",
    "[5]: https://arxiv.org/abs/1501.00102?utm_source=chatgpt.com \"ModDrop: adaptive multi-modal gesture recognition\"\n",
    "[6]: https://openaccess.thecvf.com/content/CVPR2022/papers/Alfasly_Learnable_Irrelevant_Modality_Dropout_for_Multimodal_Action_Recognition_on_Modality-Specific_CVPR_2022_paper.pdf?utm_source=chatgpt.com \"Learnable Irrelevant Modality Dropout for Multimodal ...\"\n",
    "[7]: https://arxiv.org/abs/2107.00135?utm_source=chatgpt.com \"[2107.00135] Attention Bottlenecks for Multimodal Fusion\"\n",
    "[8]: https://research.google/blog/multimodal-bottleneck-transformer-mbt-a-new-model-for-modality-fusion/?utm_source=chatgpt.com \"Multimodal Bottleneck Transformer (MBT): A New Model for ...\"\n",
    "[9]: https://arxiv.org/abs/1807.03748?utm_source=chatgpt.com \"Representation Learning with Contrastive Predictive Coding\"\n"
   ],
   "id": "17ec607b48f7a864"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1aa5fda3100e0a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Totally—EEG-SSL just means: add a small, **EEG-only self-supervised loss** that forces your EEG pathway to carry information even when video/audio/text are missing. Here’s a compact, practical menu and how to wire it into your current model.\n",
    "\n",
    "# What “EEG-SSL” is (and why)\n",
    "\n",
    "Self-supervised objectives on **EEG tokens alone** (before fusion) so your fused encoder can’t solve the task just by copying the teachers (video/audio/text). A recent survey catalogs three main families—**contrastive**, **predictive**, and **reconstruction**—all shown to help EEG with scarce labels. ([arXiv][1])\n",
    "\n",
    "# Drop-in objectives you can use\n",
    "\n",
    "### 1) Contrastive on augmented views (SimCLR/TS-TCC style)\n",
    "\n",
    "* Make two *views* of the same EEG window via benign augmentations (jitter, scaling, light time-warp, small channel-drop).\n",
    "* Encode both; L2-normalize; apply InfoNCE/NT-Xent across the batch (positives = same window; negatives = others).\n",
    "* TS-TCC adds a temporal prediction head that improves timestamp-level features on time series. Use it if you want better per-timestep $(T,D)$ quality. ([arXiv][2], [IJCAI][3])\n",
    "\n",
    "**When:** you want general representations without a decoder; works great with your sequence outputs.\n",
    "**Augmentations:** keep them mild for EEG (see “Augmentations” below). ([arXiv][4])\n",
    "\n",
    "### 2) Predictive coding across time (CPC)\n",
    "\n",
    "* Split a window into context (past) and future; encode both; train the context to **predict future latents** at several steps ahead using InfoNCE with in-batch negatives.\n",
    "* Encourages the model to encode temporal dynamics that matter in EEG rhythms. CPC variants have been used directly on EEG. ([PubMed][5])\n",
    "\n",
    "**When:** you care about temporal structure/forecastability in $T$.\n",
    "\n",
    "### 3) Masked reconstruction (MAE for EEG)\n",
    "\n",
    "* Randomly **mask time spans and/or channels** in the EEG; encode visible tokens; a light decoder reconstructs the masked signal (waveform or spectrogram) with L1/MSE.\n",
    "* Very stable and label-efficient; MAE-style approaches have been demonstrated specifically for EEG. ([Apple Machine Learning Research][6], [arXiv][7])\n",
    "\n",
    "**When:** you want a simple, robust pretext; pairs well with your contrastive teacher loss.\n",
    "\n",
    "### 4) Cross-channel prediction (drop-channel objective)\n",
    "\n",
    "* Drop a subset of channels; predict them from the remaining channels (MSE in waveform or latent space).\n",
    "* Captures spatial dependencies between electrodes; lightweight and synergistic with (1–3). (General time-series SSL literature supports masked/forecasting targets.) ([arXiv][8])\n",
    "\n",
    "# How to wire it into your current training\n",
    "\n",
    "* Compute EEG tokens **before** gated cross-attention; apply 1–2 EEG-SSL objectives there.\n",
    "* Keep your multi-teacher contrastive loss on the **pooled fused** vector $z_f$ as we discussed.\n",
    "* Use **modality dropout** (sometimes drop video/audio/text entirely) so the fused head can’t ignore EEG.\n",
    "* Typical weights: `λ_eeg_ssl ∈ [0.1, 0.3]` next to your fused→teacher loss; ramp `λ_eeg_ssl` up during the first 10–20% of training.\n",
    "\n",
    "# Safe EEG augmentations (for contrastive views)\n",
    "\n",
    "Use small magnitudes to preserve physiology:\n",
    "\n",
    "* **Jitter (additive Gaussian noise)**, **scaling** (±10%), **time-warping** (±5–10%), **window cropping**, **channel dropout** (10–20%), and **band-limited filtering** (random narrow band-pass within plausible EEG ranges).\n",
    "* If you work in spectrograms, **frequency/time masking** (SpecAugment-style) is effective. Avoid channel shuffles (montage semantics). ([arXiv][4])\n",
    "\n",
    "# Minimal sketches\n",
    "\n",
    "**A) TS-style contrastive on EEG tokens**\n",
    "\n",
    "* Two augmented views $x^{(1)}, x^{(2)}$ → enc → $z^{(1)}, z^{(2)} \\in \\mathbb{R}^{B\\times T\\times D}$\n",
    "* Pool over $T$ or do token-wise contrast (TS-TCC does both); InfoNCE with temperature $\\tau$. ([arXiv][2])\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{EEG-CL}} = \\frac{1}{B}\\sum_i\n",
    "-\\log \\frac{\\exp(\\,z_i^{(1)}\\!\\cdot z_i^{(2)}/\\tau\\,)}\n",
    "{\\sum_j \\exp(\\,z_i^{(1)}\\!\\cdot z_j^{(2)}/\\tau\\,)}\n",
    "$$\n",
    "\n",
    "**B) EEG-MAE (mask spans/channels)**\n",
    "\n",
    "* Random mask $M$ over $(T,\\text{channels})$; encoder sees only visible; decoder reconstructs masked:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MAE}} = \\| \\hat{x}_{\\text{masked}} - x_{\\text{masked}}\\|_1\n",
    "$$\n",
    "\n",
    "Effective on EEG with transformer backbones. ([Apple Machine Learning Research][6])\n",
    "\n",
    "# Quick starter recipe for you\n",
    "\n",
    "1. Add **EEG-MAE** (mask 30–60% tokens; L1) on the EEG encoder outputs (pre-fusion). ([Apple Machine Learning Research][6])\n",
    "2. Add **TS-TCC-style** contrastive on two augmented EEG views (mild jitter/warp, channel-drop). ([arXiv][2])\n",
    "3. Keep your **fused → {video,audio,text}** SigLIP losses.\n",
    "4. Enable **modality dropout** on KV streams during training 30–50% of the time.\n",
    "5. Loss: `L = L_fused_teachers + 0.2*L_EEG_MAE + 0.1*L_EEG_CL`.\n",
    "\n",
    "This combo is simple, stable, and makes the EEG pathway carry its own signal while still leveraging the teachers.\n",
    "\n",
    "**Good further reading:** EEG-SSL surveys (2024–2025) synthesize many variants and report strong label-efficiency gains; TS2Vec is an especially handy general-purpose time-series SSL you can adapt if you want hierarchical timestamp features. ([arXiv][1], [ACM Digital Library][9])\n",
    "\n",
    "[1]: https://arxiv.org/html/2401.05446v1?utm_source=chatgpt.com \"Self-supervised Learning for Electroencephalogram: A Systematic Survey\"\n",
    "[2]: https://arxiv.org/abs/2106.14112?utm_source=chatgpt.com \"Time-Series Representation Learning via Temporal and Contextual Contrasting\"\n",
    "[3]: https://www.ijcai.org/proceedings/2021/0324.pdf?utm_source=chatgpt.com \"Time-Series Representation Learning via Temporal and ...\"\n",
    "[4]: https://arxiv.org/pdf/2306.10125?utm_source=chatgpt.com \"Self-Supervised Learning for Time Series Analysis\"\n",
    "[5]: https://pubmed.ncbi.nlm.nih.gov/37990998/?utm_source=chatgpt.com \"Self-Supervised EEG Representation Learning with Contrastive Predictive Coding for Post-Stroke Patients\"\n",
    "[6]: https://machinelearning.apple.com/research/masked-auto-encoder?utm_source=chatgpt.com \"MAEEG: Masked Auto-encoder for EEG Representation ...\"\n",
    "[7]: https://arxiv.org/abs/2408.05375?utm_source=chatgpt.com \"Enhancing Representation Learning of EEG Data with Masked Autoencoders\"\n",
    "[8]: https://arxiv.org/abs/2106.10466?utm_source=chatgpt.com \"TS2Vec: Towards Universal Representation of Time Series\"\n",
    "[9]: https://dl.acm.org/doi/10.1145/3736574?utm_source=chatgpt.com \"Self-supervised Learning for Electroencephalogram: A Systematic Survey\"\n"
   ],
   "id": "4bd2847efffa5ab8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Hot theme here",
   "id": "b84d8eb98e422c52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Your friend’s take is **mostly right**—but it’s only a problem if you use *plain* reconstruction and give it too much weight. Reconstruction encourages “keep everything,” which can dampen **specialization**; KD/contrastive pushes **semantics**. The trick is to wire the objectives so they *pull in complementary directions* instead of fighting.\n",
    "\n",
    "Here’s a concrete way to tackle it.\n",
    "\n",
    "# What to keep / what to change\n",
    "\n",
    "**Avoid plain full reconstruction.**\n",
    "It rewards identity mappings and “safe compression.”\n",
    "\n",
    "**Prefer these instead (EEG-focused):**\n",
    "\n",
    "1. **Masked / denoising EEG recon (MAE-style, pre-fusion):**\n",
    "   Mask 40–60% time steps/channels; reconstruct only the masked part with a *small* decoder. This teaches structure, not copying.\n",
    "2. **Cross-modal denoising:**\n",
    "   Mask EEG input → fuse with video/audio/text → reconstruct the masked EEG. This *forces* the fusion to use both EEG and KV signals.\n",
    "3. **Predictive (CPC/next-step) EEG SSL:**\n",
    "   Predict future EEG latents with InfoNCE. This drives temporal structure without demanding pixel-perfect copies.\n",
    "\n",
    "These preserve information **useful** to EEG while letting the KD loss carve **semantics**.\n",
    "\n",
    "# Make specialization explicit (so modalities complement, not duplicate)\n",
    "\n",
    "Add one or two of these light regularizers:\n",
    "\n",
    "* **Modality dropout (strongest):** randomly drop each KV stream (sometimes all) and still apply KD. Prevents KV-only shortcuts; makes EEG carry load.\n",
    "* **EEG-only KD step:** a separate forward with KVs dropped; align fused→teachers. Guarantees the model learns EEG→teacher mappings.\n",
    "* **Capacity bottleneck on K/V:** resample video/audio/text to \\~16–64 latents total; add L1 on gates. Limits “pipe the teacher through.”\n",
    "* **Orthogonality / diversity penalty:** encourage complementary contributions from EEG vs KVs. Example:\n",
    "\n",
    "  ```python\n",
    "  # z_fused = fused rep; p_eeg, p_kv are small linear probes on the fused tokens\n",
    "  d = F.normalize(p_eeg(z_fused), dim=-1)\n",
    "  c = F.normalize(p_kv(z_fused),  dim=-1)\n",
    "  L_div = (d * c).pow(2).mean()   # push EEG- and KV-driven subspaces apart\n",
    "  ```\n",
    "* **Mismatched-KV negative:** shuffle KVs across samples for a second pass and *penalize* alignment to teachers. If EEG is ignored, this term spikes.\n",
    "\n",
    "# A simple, robust recipe\n",
    "\n",
    "**Objective**\n",
    "\n",
    "$$\n",
    "\\underbrace{\\beta\\!\\!\\sum_m\\! \\text{SigLIP}(z_f,\\ t_m)}_{\\text{KD to video/audio/text}}\n",
    "+\\ \\underbrace{\\alpha\\,\\mathcal{L}_{\\text{EEG-SSL}}}_{\\text{masked recon or CPC}}\n",
    "+\\ \\underbrace{\\lambda_{\\text{drop}}\\,\\sum_m \\text{SigLIP}(z_f^{\\text{EEG-only}}, t_m)}_{\\text{EEG-only KD}}\n",
    "+\\ \\underbrace{\\gamma\\,L_{\\text{div}} + \\rho\\,L_{\\text{gate}}}_{\\text{complementarity \\& anti-shortcut}}\n",
    "$$\n",
    "\n",
    "**Wiring**\n",
    "\n",
    "* Put EEG-SSL **on a tiny trainable EEG adapter right before Q** (queries) so it actually updates something upstream of fusion.\n",
    "* Use **masked** EEG recon (or CPC) rather than full recon.\n",
    "* **Drop** KVs 30–50% of steps; keep a small **EEG-only KD** term.\n",
    "* Keep **per-modality temperatures/biases** for SigLIP.\n",
    "\n",
    "**Weights to start**\n",
    "\n",
    "* $\\beta=1.0$, $\\alpha=0.2$, $\\lambda_{\\text{drop}}=0.3$, $\\gamma=1e{-3}$, $\\rho=1e{-4}$.\n",
    "  Warm up $\\beta$ over the first 10–20% steps; optionally decay $\\alpha$ a bit later.\n",
    "\n",
    "# How you’ll know it’s working\n",
    "\n",
    "* **Ablations:**\n",
    "  – Drop KVs → performance dips but stays > chance.\n",
    "  – Drop EEG → noticeable drop (EEG matters).\n",
    "* **Gate/attn stats:** gates not pegged; attention entropy reasonable.\n",
    "* **Saliency:** non-trivial grads wrt EEG inputs.\n",
    "* **Mismatched-KV test:** hurts alignment (means EEG contributes).\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom line:** Your friend is right about the *risk*—plain reconstruction can blunt specialization. Use **masked/cross-modal** EEG SSL, **modality dropout + EEG-only KD**, and a **small diversity regularizer**. That combination keeps EEG informative, lets KD sculpt semantics, and encourages **complementary** (not redundant) cross-modal representations.\n"
   ],
   "id": "90d5a599c6e002e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model import SimpleEEGAVI\n",
    "\n",
    "mod = SimpleEEGAVI(384)"
   ],
   "id": "1bebd5242fa438de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9c4afee71bd860cc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
