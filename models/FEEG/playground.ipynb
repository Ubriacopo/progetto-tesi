{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:21:05.495088Z",
     "start_time": "2025-08-21T13:21:00.982552Z"
    }
   },
   "source": [
    "# Imports block\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "from transformers import VivitImageProcessor, AutoProcessor\n",
    "from models.FEEG.base_embedding import ViViTFoundationEmbedder, MiniLMFoundationEmbedder\n",
    "from common.amigos.dataset import AMIGOSDataset\n",
    "from models.FEEG.layers import PerceiverResampler, SimplePerceiverResampler\n",
    "from models.FEEG.model import EEGAVI\n",
    "from einops import rearrange"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jacopo/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Interaction",
   "id": "e4c56ae3bd0dda8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds = AMIGOSDataset(\"../../resources/AMIGOS/sampled/AMIGOS_sampled.csv\")\n",
    "ds.__getitem__(1)"
   ],
   "id": "8b2d4d3849d83c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "a = ds.__getitem__(1)\n",
    "for i in a:\n",
    "    if i is None:\n",
    "        print(\"None\")\n",
    "    else:\n",
    "        print(i.shape if isinstance(i, np.ndarray) else len(i))"
   ],
   "id": "dc3b119865a8da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(a[0])",
   "id": "cab6719899787645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.array(a[0]).shape",
   "id": "f8ad996cf1b7bb82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "### Structure\n",
    "We use ```make_dot``` to plot a structure of the actual model. <br>\n",
    "This step is just to see if the shapes match and there were no mistakes on that behalf."
   ],
   "id": "1d7205299df88941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ViViT\n",
    "ViViT accepts only 32 frames sequence inputs. How to operate depends on approach:\n",
    "- Uniform sampling → evenly pick 32 frames across the whole 4 s (good coverage)\n",
    "- Random sampling → randomly pick 32 frames (common in training for augmentation).\n",
    "- Sliding windows → split into multiple 32-frame clips (e.g. 120 frames → 3–4 clips of 32), process each, then average/aggregate.\n",
    "\n",
    "> Sliding window could be what I need for face expressions altough more costy as I need to compute multiple times ViVIT downstream when feeding the video."
   ],
   "id": "fd72d97ee20bf079"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:21:48.828959Z",
     "start_time": "2025-08-21T13:21:43.745235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vivit = ViViTFoundationEmbedder()\n",
    "video = torch.randint(low=0, high=256, size=(32, 3, 224, 224))\n",
    "\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "a = processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "non_reshaped = vivit.base_model(a.pixel_values)\n",
    "r = vivit(**a)"
   ],
   "id": "80c70c400ad3dee9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  ViViT + Perceiver",
   "id": "392b61533d004bae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:25.092659Z",
     "start_time": "2025-08-21T13:22:24.843584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Shape of video after ViViT:\" + str(r.shape))\n",
    "other = SimplePerceiverResampler(768, 2)(non_reshaped.last_hidden_state)\n",
    "print(\"Shape of processed after simple\" + str(other.shape))\n",
    "\n",
    "resampler_video = PerceiverResampler(768, 2, max_num_frames=16, max_num_media=None)(r)\n",
    "print(\"After OpenFlamingo: \" + str(resampler_video.shape))"
   ],
   "id": "e474b137c7afec0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of video after ViViT:torch.Size([1, 32, 1, 98, 768])\n",
      "Shape of processed after simpletorch.Size([1, 1, 64, 768])\n",
      "After OpenFlamingo: torch.Size([1, 32, 64, 768])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:26.359813Z",
     "start_time": "2025-08-21T13:22:26.356766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fuse the time steps with the latent dimension space\n",
    "R_v = rearrange(resampler_video, \"b t l d -> b (t l) d\")\n",
    "R_v.shape"
   ],
   "id": "3361a45bc0dd8b29",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f8ceefc976feb22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WavLM",
   "id": "8818520364b8013e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:31.796709Z",
     "start_time": "2025-08-21T13:22:30.904527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.FEEG.base_embedding import WavLMFoundationEmbedder\n",
    "\n",
    "audio = torch.randn(1, 16000)\n",
    "wavlm = WavLMFoundationEmbedder()"
   ],
   "id": "49f086adc4bd9f59",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:32.536037Z",
     "start_time": "2025-08-21T13:22:32.407100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = wavlm(audio)\n",
    "print(\"Shape after WavLM: \" + str(y.shape))\n",
    "resampled = PerceiverResampler(768, 2)(y)\n",
    "print(\"Simple perceiver:\" + str(resampled.shape))"
   ],
   "id": "bb53d739db3cf8ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after WavLM: torch.Size([1, 1, 49, 1, 768])\n",
      "Simple perceiver:torch.Size([1, 1, 64, 768])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:38.641241Z",
     "start_time": "2025-08-21T13:22:38.638135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_a = rearrange(resampled, \"b t l d -> b (t l) d\")\n",
    "R_a.shape"
   ],
   "id": "bb72709c905b58a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:39.033478Z",
     "start_time": "2025-08-21T13:22:39.029925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The gated attn is fed with the concatenation of the aux embeddings.\n",
    "torch.cat([R_v, R_a], dim=1).shape"
   ],
   "id": "678c24e4e65e7e08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2112, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MiniLM",
   "id": "4411085b4f564e1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:41.793588Z",
     "start_time": "2025-08-21T13:22:41.631601Z"
    }
   },
   "cell_type": "code",
   "source": "minilm = MiniLMFoundationEmbedder()",
   "id": "433ebf8acee409dd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:42.555007Z",
     "start_time": "2025-08-21T13:22:42.396981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "inputs = \"This is a text test\"\n",
    "minilm_processor = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "a = minilm_processor(inputs, padding=True, truncation=True, return_tensors='pt')"
   ],
   "id": "ce46d3f59504c6ad",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:43.335501Z",
     "start_time": "2025-08-21T13:22:43.331916Z"
    }
   },
   "cell_type": "code",
   "source": "a",
   "id": "26ecefcbf31661ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3793, 3231,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:22:49.142590Z",
     "start_time": "2025-08-21T13:22:49.132067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = minilm(**a)\n",
    "res.shape"
   ],
   "id": "a36d2e2ad0cbbf7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7, 1, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:23:21.557530Z",
     "start_time": "2025-08-21T13:23:21.535716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "text_resampled = PerceiverResampler(384, 2)(res)\n",
    "R_t = rearrange(text_resampled, \"b t l d -> b (t l) d\")\n",
    "print(R_t.shape)  # Shape mismatch for building the input\n",
    "\n",
    "# For mismatch simply project to correct feature space.\n",
    "o = nn.Linear(384, 768)(R_t)\n",
    "o.shape"
   ],
   "id": "42f7450a1d7b946a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Model",
   "id": "61184b75efba1317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = EEGAVI()  # Model initialization\n",
    "\n",
    "# Build input x as:\n",
    "x_vid = torch.randn(2, 32, 3, 224, 224)\n",
    "x_aud = torch.randn(2, 16000)\n",
    "x_tex = None  # We try without text\n",
    "\n",
    "# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)\n",
    "x_eeg = torch.randn(8, 22, 4, 200)\n",
    "\n",
    "y = model((x_eeg, x_vid, x_aud, x_tex))\n",
    "\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)"
   ],
   "id": "5c02239400a7acea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
