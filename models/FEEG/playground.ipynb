{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:59:41.405099Z",
     "start_time": "2025-08-21T19:59:34.694022Z"
    }
   },
   "source": [
    "# Imports block\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torchviz import make_dot\n",
    "from transformers import VivitImageProcessor, AutoFeatureExtractor\n",
    "\n",
    "from common.amigos.dataset import AMIGOSDataset\n",
    "from models.FEEG.base_embedding import ViViTFoundationEmbedder, MiniLMFoundationEmbedder, CBraModFoundationEmbedder\n",
    "from models.FEEG.layers import PerceiverResampler\n",
    "from models.FEEG.model import EEGAVI"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jacopo/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Interaction",
   "id": "e4c56ae3bd0dda8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds = AMIGOSDataset(\"../../resources/AMIGOS/sampled/AMIGOS_sampled.csv\")\n",
    "ds.__getitem__(1)"
   ],
   "id": "8b2d4d3849d83c32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "a = ds.__getitem__(1)\n",
    "for i in a:\n",
    "    if i is None:\n",
    "        print(\"None\")\n",
    "    else:\n",
    "        print(i.shape if isinstance(i, np.ndarray) else len(i))"
   ],
   "id": "dc3b119865a8da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(a[0])",
   "id": "cab6719899787645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.array(a[0]).shape",
   "id": "f8ad996cf1b7bb82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "### Structure\n",
    "We use ```make_dot``` to plot a structure of the actual model. <br>\n",
    "This step is just to see if the shapes match and there were no mistakes on that behalf."
   ],
   "id": "1d7205299df88941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ViViT\n",
    "ViViT accepts only 32 frames sequence inputs. How to operate depends on approach:\n",
    "- Uniform sampling → evenly pick 32 frames across the whole 4 s (good coverage)\n",
    "- Random sampling → randomly pick 32 frames (common in training for augmentation).\n",
    "- Sliding windows → split into multiple 32-frame clips (e.g. 120 frames → 3–4 clips of 32), process each, then average/aggregate.\n",
    "\n",
    "> Sliding window could be what I need for face expressions altough more costy as I need to compute multiple times ViVIT downstream when feeding the video."
   ],
   "id": "fd72d97ee20bf079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vivit = ViViTFoundationEmbedder()\n",
    "video = torch.randint(low=0, high=256, size=(32, 3, 224, 224))\n",
    "\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "a = processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "non_reshaped = vivit.base_model(a.pixel_values)\n",
    "r = vivit(**a)"
   ],
   "id": "80c70c400ad3dee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  ViViT + Perceiver",
   "id": "392b61533d004bae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Shape of video after ViViT:\" + str(r.shape))\n",
    "resampler_video = PerceiverResampler(768, 2, max_num_frames=16, max_num_media=None)(r)\n",
    "print(\"After OpenFlamingo: \" + str(resampler_video.shape))"
   ],
   "id": "e474b137c7afec0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fuse the time steps with the latent dimension space\n",
    "R_v = rearrange(resampler_video, \"b t l d -> b (t l) d\")\n",
    "R_v.shape"
   ],
   "id": "3361a45bc0dd8b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f8ceefc976feb22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WavLM",
   "id": "8818520364b8013e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:45:42.639534Z",
     "start_time": "2025-08-21T19:45:41.907309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.FEEG.base_embedding import W2VBertFoundationEmbedder\n",
    "\n",
    "audio_processor = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "audio = audio_processor(torch.randn(16000), padding=True, return_tensors='pt')\n",
    "wavlm = W2VBertFoundationEmbedder()"
   ],
   "id": "49f086adc4bd9f59",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:45:43.600331Z",
     "start_time": "2025-08-21T19:45:43.596591Z"
    }
   },
   "cell_type": "code",
   "source": "audio",
   "id": "7fd671dac99b3737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[-0.1592,  0.5624,  1.4083,  ...,  1.4783, -0.8547, -0.8763],\n",
       "         [ 1.2123, -1.8793, -0.4854,  ...,  0.2755,  0.4648, -0.5399],\n",
       "         [ 0.4843,  0.2714, -0.1503,  ...,  0.3840, -0.2497, -0.9086],\n",
       "         ...,\n",
       "         [-0.4710, -0.8578,  0.9055,  ...,  0.2753,  0.2492,  1.8739],\n",
       "         [ 0.1926,  0.4783,  0.3498,  ..., -0.8448, -2.1798,  1.5988],\n",
       "         [ 0.2215,  0.0423, -0.0263,  ..., -0.2740,  0.4701,  0.1755]]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:45:56.149213Z",
     "start_time": "2025-08-21T19:45:55.706445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = wavlm(**audio)\n",
    "print(\"Shape after WavLM: \" + str(y.shape))\n",
    "resampled = PerceiverResampler(1024, 2)(y)\n",
    "print(\"Simple perceiver:\" + str(resampled.shape))"
   ],
   "id": "bb53d739db3cf8ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after WavLM: torch.Size([1, 1, 49, 1, 1024])\n",
      "Simple perceiver:torch.Size([1, 1, 64, 1024])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:46:09.248532Z",
     "start_time": "2025-08-21T19:46:09.245746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_a = rearrange(resampled, \"b t l d -> b (t l) d\")\n",
    "R_a.shape"
   ],
   "id": "bb72709c905b58a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:46:11.779553Z",
     "start_time": "2025-08-21T19:46:11.759660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The gated attn is fed with the concatenation of the aux embeddings.\n",
    "torch.cat([R_v, R_a], dim=1).shape"
   ],
   "id": "678c24e4e65e7e08",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'R_v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# The gated attn is fed with the concatenation of the aux embeddings.\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m torch.cat([\u001B[43mR_v\u001B[49m, R_a], dim=\u001B[32m1\u001B[39m).shape\n",
      "\u001B[31mNameError\u001B[39m: name 'R_v' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MiniLM",
   "id": "4411085b4f564e1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "minilm = MiniLMFoundationEmbedder()",
   "id": "433ebf8acee409dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "inputs = \"This is a text test\"\n",
    "minilm_processor = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "a = minilm_processor(inputs, padding=True, truncation=True, return_tensors='pt')"
   ],
   "id": "ce46d3f59504c6ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a",
   "id": "26ecefcbf31661ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = minilm(**a)\n",
    "res.shape"
   ],
   "id": "a36d2e2ad0cbbf7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "text_resampled = PerceiverResampler(384, 2)(res)\n",
    "R_t = rearrange(text_resampled, \"b t l d -> b (t l) d\")\n",
    "print(R_t.shape)  # Shape mismatch for building the input\n",
    "\n",
    "# For mismatch simply project to correct feature space.\n",
    "o = nn.Linear(384, 768)(R_t)\n",
    "o.shape"
   ],
   "id": "42f7450a1d7b946a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CBraMod\n",
   "id": "9222992b2dda7033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:00:00.419576Z",
     "start_time": "2025-08-21T19:00:00.339580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cbramod = CBraModFoundationEmbedder()\n",
    "# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)\n",
    "x_eeg = torch.randn(1, 22, 4, 200)\n",
    "res = cbramod(x=x_eeg)"
   ],
   "id": "a2077d18d388a1bf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T19:01:23.216905Z",
     "start_time": "2025-08-21T19:01:23.214427Z"
    }
   },
   "cell_type": "code",
   "source": "res.shape",
   "id": "2ef078a970b4d818",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 4, 200])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "text_resampled = PerceiverResampler(384, 2)(res)",
   "id": "4f5f8745134f5f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Model",
   "id": "1802f03537b89ca0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T20:02:47.929816Z",
     "start_time": "2025-08-21T19:59:45.248185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model initialization\n",
    "model = EEGAVI(\n",
    "    resampler_depth=2,\n",
    "    text_kd_size=600,\n",
    "    video_kd_size=600,\n",
    "    audio_kd_size=600\n",
    ")\n",
    "\n",
    "processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "# Build input x as:\n",
    "video = torch.randint(low=0, high=256, size=(32, 3, 224, 224))\n",
    "x_vid = processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "x_aud = processor(torch.randn(1, 16000), return_tensors=\"pt\")\n",
    "\n",
    "x_tex = None  # We try without text\n",
    "\n",
    "# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)\n",
    "x_eeg = torch.randn(1, 22, 4, 200)\n",
    "\n",
    "y = model(({\"x\":x_eeg}, x_vid, x_aud, x_tex))\n",
    "\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)"
   ],
   "id": "5c02239400a7acea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `SeamlessM4TFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# mock_eeg.shape = (batch_size, num_of_channels, time_segments, points_per_patch)\u001B[39;00m\n\u001B[32m     20\u001B[39m x_eeg = torch.randn(\u001B[32m1\u001B[39m, \u001B[32m22\u001B[39m, \u001B[32m4\u001B[39m, \u001B[32m200\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m y = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mx\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43mx_eeg\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_vid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_aud\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_tex\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m make_dot(y.mean(), params=\u001B[38;5;28mdict\u001B[39m(model.named_parameters()), show_attrs=\u001B[38;5;28;01mTrue\u001B[39;00m, show_saved=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/models/FEEG/model.py:141\u001B[39m, in \u001B[36mEEGAVI.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    137\u001B[39m embeddings = torch.cat([x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m [ve, ae, te] \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m], dim=\u001B[32m1\u001B[39m)\n\u001B[32m    139\u001B[39m \u001B[38;5;66;03m# TODO: la gated attn non ha shape fissa?\u001B[39;00m\n\u001B[32m    140\u001B[39m \u001B[38;5;66;03m# Now we do Cross-Attention + Gating\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m ee = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgatedXAttn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mee\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    142\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.projector(ee)\n\u001B[32m    143\u001B[39m \u001B[38;5;66;03m# Final projection head?\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/models/FEEG/layers.py:257\u001B[39m, in \u001B[36mGatedCrossAttentionBlock.forward\u001B[39m\u001B[34m(self, q, kv, media_locations, use_cached_media)\u001B[39m\n\u001B[32m    255\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, q, kv, media_locations=\u001B[38;5;28;01mNone\u001B[39;00m, use_cached_media=\u001B[38;5;28;01mFalse\u001B[39;00m, ):\n\u001B[32m    256\u001B[39m     use_cached_media = \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# To see if it works at 0. Poi sara da fare\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m257\u001B[39m     q = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmedia_locations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cached_media\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m * \u001B[38;5;28mself\u001B[39m.attn_gate.tanh() + q\n\u001B[32m    258\u001B[39m     q = \u001B[38;5;28mself\u001B[39m.ff(q) * \u001B[38;5;28mself\u001B[39m.ff_gate.tanh() + q  \u001B[38;5;66;03m# Residual network\u001B[39;00m\n\u001B[32m    259\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m q\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/progetto-tesi/models/FEEG/layers.py:204\u001B[39m, in \u001B[36mMaskedCrossAttention.forward\u001B[39m\u001B[34m(self, x, media, media_locations, use_cached_media)\u001B[39m\n\u001B[32m    202\u001B[39m k, v = \u001B[38;5;28mself\u001B[39m.to_kv(kv_object).chunk(\u001B[32m2\u001B[39m, dim=-\u001B[32m1\u001B[39m)\n\u001B[32m    203\u001B[39m \u001B[38;5;66;03m# TODO: Checkpoint\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m204\u001B[39m q, k, v = \u001B[43mrearrange_many\u001B[49m((q, k, v), \u001B[33m\"\u001B[39m\u001B[33mb n (h d) -> b h n d\u001B[39m\u001B[33m\"\u001B[39m, h=\u001B[38;5;28mself\u001B[39m.heads)\n\u001B[32m    205\u001B[39m q *= \u001B[38;5;28mself\u001B[39m.scale  \u001B[38;5;66;03m# Rescale the query\u001B[39;00m\n\u001B[32m    207\u001B[39m \u001B[38;5;66;03m# Check similarity between key and query\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:892\u001B[39m, in \u001B[36mPyDBFrame.trace_dispatch\u001B[39m\u001B[34m(self, frame, event, arg)\u001B[39m\n\u001B[32m    890\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_line:\n\u001B[32m    891\u001B[39m     \u001B[38;5;28mself\u001B[39m.set_suspend(thread, step_cmd)\n\u001B[32m--> \u001B[39m\u001B[32m892\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# return event\u001B[39;00m\n\u001B[32m    894\u001B[39m     back = frame.f_back\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[39m, in \u001B[36mPyDBFrame.do_wait_suspend\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    411\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m412\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm/plugins/python-ce/helpers/pydev/pydevd.py:1196\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1193\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1195\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1196\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm/plugins/python-ce/helpers/pydev/pydevd.py:1211\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1208\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1210\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1211\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1213\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1215\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
